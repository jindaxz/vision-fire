{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad6e204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Nov_30_19:08:53_PST_2020\n",
      "Cuda compilation tools, release 11.2, V11.2.67\n",
      "Build cuda_11.2.r11.2/compiler.29373293_0\n",
      "gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "Sun Feb 25 18:50:27 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   65C    P0              65W /  70W |   9267MiB / 15360MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    128900      C   python                                     9262MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "/work/van-speech-nlp/jindaznb/j-vis\n",
      "System Prefix: /work/van-speech-nlp/jenv\n",
      "HOME: /work/van-speech-nlp/jindaznb/j-vis\n",
      "2.0.0+cu117 True\n",
      "3.3.0\n",
      "11.7\n",
      "GCC 9.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 18:50:47.211354: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-25 18:50:47.259299: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-25 18:50:47.259330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-25 18:50:47.260467: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-25 18:50:47.268470: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-25 18:50:50.256045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set seed successfully\n"
     ]
    }
   ],
   "source": [
    "%run 1-0mmyolo-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c81a8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Write custom Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d024c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/j-vis/mmdetection\n"
     ]
    }
   ],
   "source": [
    "# %cd {HOME}/mmyolo\n",
    "%cd {HOME}/mmdetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dfeef84-c3fb-4329-ba12-9b61a5c9d535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/j-vis/mmdetection/configs/faster_rcnn/custom-faster-rcnn_x101-64x4d_fpn_ms-3x_coco.py\n"
     ]
    }
   ],
   "source": [
    "model_folder=f\"faster_rcnn\"\n",
    "base_model_name=f\"faster-rcnn_x101-64x4d_fpn_ms-3x_coco\"\n",
    "custom_model_name=f\"custom-{base_model_name}\"\n",
    "CUSTOM_CONFIG_PATH = f\"{HOME}/mmdetection/configs/{model_folder}/{custom_model_name}.py\"\n",
    "print(CUSTOM_CONFIG_PATH)\n",
    "\n",
    "\n",
    "CUSTOM_CONFIG = f\"\"\"\n",
    "_base_ = './{base_model_name}.py'\n",
    "\n",
    "# ========================Frequently modified parameters======================\n",
    "# -----data related-----\n",
    "model = dict(\n",
    "    roi_head=dict(\n",
    "        bbox_head=dict(num_classes={num_classes}),\n",
    "))\n",
    "\n",
    "dataset_type = 'COCODataset'\n",
    "classes =  {classes}\n",
    "\n",
    "data_root = '{dataset_location}' # Root directory of the dataset\n",
    "\n",
    "train_ann_file = 'train/_annotations.coco.json'  # Annotation file for training set\n",
    "train_data_prefix = 'train/'  # Prefix for training data directory\n",
    "\n",
    "val_ann_file = 'valid/_annotations.coco.json'  # Annotation file for validation set\n",
    "val_data_prefix = 'valid/'  # Prefix for validation data directory\n",
    "\n",
    "class_name = {classes} \n",
    "num_classes = {num_classes}  # Number of classes in the dataset\n",
    "metainfo = dict(classes=class_name, palette=[(20, 220, 60)])  # Metadata information for visualization\n",
    "\n",
    "train_batch_size_per_gpu = {BATCH_SIZE}  # Batch size per GPU during training\n",
    "#train_num_workers = 4  # Number of worker processes for data loading during training\n",
    "persistent_workers = True  # Whether to use persistent workers during training\n",
    "\n",
    "# -----train val related-----\n",
    "# base_lr = 0.004  # Base learning rate for optimization\n",
    "base_lr = 0.04\n",
    "max_epochs = {MAX_EPOCHS}  # Maximum training epochs\n",
    "num_epochs_stage2 = 20  # Number of epochs for stage 2 training\n",
    "\n",
    "model_test_cfg = dict(\n",
    "    multi_label=True,  # Multi-label configuration for multi-class prediction\n",
    "    nms_pre=30000,  # Number of boxes before NMS\n",
    "    score_thr=0.001,  # Score threshold to filter out boxes\n",
    "    nms=dict(type='nms', iou_threshold=0.65),  # NMS type and threshold\n",
    "    max_per_img=300)  # Maximum number of detections per image\n",
    "\n",
    "\n",
    "# ========================Possible modified parameters========================\n",
    "default_hooks = dict(\n",
    "    checkpoint=dict(\n",
    "        type=\"CheckpointHook\",\n",
    "        save_best=\"coco/bbox_mAP_50\",\n",
    "        rule=\"greater\",\n",
    "        max_keep_ckpts=10,\n",
    "    ),\n",
    "    early_stopping=dict(\n",
    "        type=\"EarlyStoppingHook\",\n",
    "        monitor=\"coco/bbox_mAP_50\",\n",
    "        patience=15,\n",
    "        min_delta=0.001\n",
    "    ),\n",
    ")\n",
    "            \n",
    "\n",
    "train_cfg=dict(\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "data = dict(\n",
    "    #samples_per_gpu=1,\n",
    "    #workers_per_gpu=2,\n",
    "    train=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='train/',\n",
    "        classes=classes,\n",
    "        ann_file='train/_annotations.coco.json.json'),\n",
    "    val=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='valid/',\n",
    "        classes=classes,\n",
    "        ann_file='valid/_annotations.coco.json'),\n",
    "    test=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='test/',\n",
    "        classes=classes,\n",
    "        ann_file='test/_annotations.coco.json'))\n",
    "\n",
    "# -----data related-----\n",
    "img_scale = (1024, 1024)  # width, height\n",
    "# ratio range for random resize\n",
    "random_resize_ratio_range = (0.1, 2.0)\n",
    "# Cached images number in mosaic\n",
    "mosaic_max_cached_images = 40\n",
    "# Number of cached images in mixupep\n",
    "mixup_max_cached_images = 20\n",
    "# Batch size of a single GPU during validation\n",
    "val_batch_size_per_gpu = 8\n",
    "# Worker to pre-fetch data for each single GPU during validation\n",
    "val_num_workers = 3\n",
    "\n",
    "# Config of batch shapes. Only on val.\n",
    "batch_shapes_cfg = dict(\n",
    "    type='BatchShapePolicy',\n",
    "    batch_size=val_batch_size_per_gpu,\n",
    "    img_size=img_scale[0],\n",
    "    size_divisor=32,\n",
    "    extra_pad_ratio=0.5)\n",
    "\n",
    "# -----train val related-----\n",
    "lr_start_factor = 1.0e-5\n",
    "dsl_topk = 13  # Number of bbox selected in each level\n",
    "loss_cls_weight = 1.0\n",
    "loss_bbox_weight = 2.0\n",
    "qfl_beta = 2.0  # beta of QualityFocalLoss\n",
    "weight_decay = 0.05\n",
    "\n",
    "# Save model checkpoint and validation intervals\n",
    "save_checkpoint_intervals = 10\n",
    "# validation intervals in stage 2\n",
    "val_interval_stage2 = 1\n",
    "# The maximum checkpoints to keep.\n",
    "max_keep_ckpts = 3\n",
    "# single-scale training is recommended to\n",
    "# be turned on, which can speed up training.\n",
    "env_cfg = dict(cudnn_benchmark=True)\n",
    "\"\"\"\n",
    "with open(CUSTOM_CONFIG_PATH, 'w') as file:\n",
    "    file.write(CUSTOM_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbe8b936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/25 19:13:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1039662955\n",
      "    GPU 0: Tesla T4\n",
      "    CUDA_HOME: /shared/centos7/cuda/11.2\n",
      "    NVCC: Cuda compilation tools, release 11.2, V11.2.67\n",
      "    GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\n",
      "    PyTorch: 2.0.0+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.15.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.10.2\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: True\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1039662955\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "02/25 19:13:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=16, enable=False)\n",
      "backend_args = None\n",
      "base_lr = 0.04\n",
      "batch_shapes_cfg = dict(\n",
      "    batch_size=8,\n",
      "    extra_pad_ratio=0.5,\n",
      "    img_size=1024,\n",
      "    size_divisor=32,\n",
      "    type='BatchShapePolicy')\n",
      "class_name = (\n",
      "    'Alive Tree',\n",
      "    'Beetle-Fire Tree',\n",
      "    'Dead Tree',\n",
      "    'Debris',\n",
      ")\n",
      "classes = (\n",
      "    'Alive Tree',\n",
      "    'Beetle-Fire Tree',\n",
      "    'Dead Tree',\n",
      "    'Debris',\n",
      ")\n",
      "data = dict(\n",
      "    test=dict(\n",
      "        ann_file='test/_annotations.coco.json',\n",
      "        classes=(\n",
      "            'Alive Tree',\n",
      "            'Beetle-Fire Tree',\n",
      "            'Dead Tree',\n",
      "            'Debris',\n",
      "        ),\n",
      "        img_prefix='test/',\n",
      "        type='COCODataset'),\n",
      "    train=dict(\n",
      "        ann_file='train/_annotations.coco.json.json',\n",
      "        classes=(\n",
      "            'Alive Tree',\n",
      "            'Beetle-Fire Tree',\n",
      "            'Dead Tree',\n",
      "            'Debris',\n",
      "        ),\n",
      "        img_prefix='train/',\n",
      "        type='COCODataset'),\n",
      "    val=dict(\n",
      "        ann_file='valid/_annotations.coco.json',\n",
      "        classes=(\n",
      "            'Alive Tree',\n",
      "            'Beetle-Fire Tree',\n",
      "            'Dead Tree',\n",
      "            'Debris',\n",
      "        ),\n",
      "        img_prefix='valid/',\n",
      "        type='COCODataset'))\n",
      "data_root = '/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1'\n",
      "dataset_type = 'COCODataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(\n",
      "        interval=1,\n",
      "        max_keep_ckpts=10,\n",
      "        rule='greater',\n",
      "        save_best='coco/bbox_mAP_50',\n",
      "        type='CheckpointHook'),\n",
      "    early_stopping=dict(\n",
      "        min_delta=0.001,\n",
      "        monitor='coco/bbox_mAP_50',\n",
      "        patience=15,\n",
      "        type='EarlyStoppingHook'),\n",
      "    logger=dict(interval=50, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "dsl_topk = 13\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=True,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "img_scale = (\n",
      "    1024,\n",
      "    1024,\n",
      ")\n",
      "launcher = 'none'\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "loss_bbox_weight = 2.0\n",
      "loss_cls_weight = 1.0\n",
      "lr_start_factor = 1e-05\n",
      "max_epochs = 200\n",
      "max_keep_ckpts = 3\n",
      "metainfo = dict(\n",
      "    classes=(\n",
      "        'Alive Tree',\n",
      "        'Beetle-Fire Tree',\n",
      "        'Dead Tree',\n",
      "        'Debris',\n",
      "    ),\n",
      "    palette=[\n",
      "        (\n",
      "            20,\n",
      "            220,\n",
      "            60,\n",
      "        ),\n",
      "    ])\n",
      "mixup_max_cached_images = 20\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        base_width=4,\n",
      "        depth=101,\n",
      "        frozen_stages=1,\n",
      "        groups=64,\n",
      "        init_cfg=dict(\n",
      "            checkpoint='open-mmlab://resnext101_64x4d', type='Pretrained'),\n",
      "        norm_cfg=dict(requires_grad=True, type='BN'),\n",
      "        norm_eval=True,\n",
      "        num_stages=4,\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        style='pytorch',\n",
      "        type='ResNeXt'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_size_divisor=32,\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='DetDataPreprocessor'),\n",
      "    neck=dict(\n",
      "        in_channels=[\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            2048,\n",
      "        ],\n",
      "        num_outs=5,\n",
      "        out_channels=256,\n",
      "        type='FPN'),\n",
      "    roi_head=dict(\n",
      "        bbox_head=dict(\n",
      "            bbox_coder=dict(\n",
      "                target_means=[\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                ],\n",
      "                target_stds=[\n",
      "                    0.1,\n",
      "                    0.1,\n",
      "                    0.2,\n",
      "                    0.2,\n",
      "                ],\n",
      "                type='DeltaXYWHBBoxCoder'),\n",
      "            fc_out_channels=1024,\n",
      "            in_channels=256,\n",
      "            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "            loss_cls=dict(\n",
      "                loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=False),\n",
      "            num_classes=4,\n",
      "            reg_class_agnostic=False,\n",
      "            roi_feat_size=7,\n",
      "            type='Shared2FCBBoxHead'),\n",
      "        bbox_roi_extractor=dict(\n",
      "            featmap_strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            out_channels=256,\n",
      "            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),\n",
      "            type='SingleRoIExtractor'),\n",
      "        type='StandardRoIHead'),\n",
      "    rpn_head=dict(\n",
      "        anchor_generator=dict(\n",
      "            ratios=[\n",
      "                0.5,\n",
      "                1.0,\n",
      "                2.0,\n",
      "            ],\n",
      "            scales=[\n",
      "                8,\n",
      "            ],\n",
      "            strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "                64,\n",
      "            ],\n",
      "            type='AnchorGenerator'),\n",
      "        bbox_coder=dict(\n",
      "            target_means=[\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "            ],\n",
      "            target_stds=[\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "            ],\n",
      "            type='DeltaXYWHBBoxCoder'),\n",
      "        feat_channels=256,\n",
      "        in_channels=256,\n",
      "        loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "        loss_cls=dict(\n",
      "            loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=True),\n",
      "        type='RPNHead'),\n",
      "    test_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            max_per_img=100,\n",
      "            nms=dict(iou_threshold=0.5, type='nms'),\n",
      "            score_thr=0.05),\n",
      "        rpn=dict(\n",
      "            max_per_img=1000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=1000)),\n",
      "    train_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=False,\n",
      "                min_pos_iou=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                pos_iou_thr=0.5,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=True,\n",
      "                neg_pos_ub=-1,\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                type='RandomSampler')),\n",
      "        rpn=dict(\n",
      "            allowed_border=-1,\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.3,\n",
      "                neg_iou_thr=0.3,\n",
      "                pos_iou_thr=0.7,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=False,\n",
      "                neg_pos_ub=-1,\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                type='RandomSampler')),\n",
      "        rpn_proposal=dict(\n",
      "            max_per_img=1000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=2000)),\n",
      "    type='FasterRCNN')\n",
      "model_test_cfg = dict(\n",
      "    max_per_img=300,\n",
      "    multi_label=True,\n",
      "    nms=dict(iou_threshold=0.65, type='nms'),\n",
      "    nms_pre=30000,\n",
      "    score_thr=0.001)\n",
      "mosaic_max_cached_images = 40\n",
      "num_classes = 4\n",
      "num_epochs_stage2 = 20\n",
      "optim_wrapper = dict(\n",
      "    optimizer=dict(lr=0.02, momentum=0.9, type='SGD', weight_decay=0.0001),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0, by_epoch=False, end=500, start_factor=0.001, type='LinearLR'),\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        end=12,\n",
      "        gamma=0.1,\n",
      "        milestones=[\n",
      "            9,\n",
      "            11,\n",
      "        ],\n",
      "        type='MultiStepLR'),\n",
      "]\n",
      "persistent_workers = True\n",
      "qfl_beta = 2.0\n",
      "random_resize_ratio_range = (\n",
      "    0.1,\n",
      "    2.0,\n",
      ")\n",
      "resume = True\n",
      "save_checkpoint_intervals = 10\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='valid/_annotations.coco.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='valid/'),\n",
      "        data_root='/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file=\n",
      "    '/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1/valid/_annotations.coco.json',\n",
      "    backend_args=None,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_ann_file = 'train/_annotations.coco.json'\n",
      "train_batch_size_per_gpu = 8\n",
      "train_cfg = dict(max_epochs=200, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_data_prefix = 'train/'\n",
      "train_dataloader = dict(\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        dataset=dict(\n",
      "            ann_file='train/_annotations.coco.json',\n",
      "            backend_args=None,\n",
      "            data_prefix=dict(img='train/'),\n",
      "            data_root='/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1',\n",
      "            filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "            pipeline=[\n",
      "                dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                dict(type='LoadAnnotations', with_bbox=True),\n",
      "                dict(\n",
      "                    keep_ratio=True,\n",
      "                    scale=[\n",
      "                        (\n",
      "                            1333,\n",
      "                            640,\n",
      "                        ),\n",
      "                        (\n",
      "                            1333,\n",
      "                            800,\n",
      "                        ),\n",
      "                    ],\n",
      "                    type='RandomResize'),\n",
      "                dict(prob=0.5, type='RandomFlip'),\n",
      "                dict(type='PackDetInputs'),\n",
      "            ],\n",
      "            type='CocoDataset'),\n",
      "        times=3,\n",
      "        type='RepeatDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        keep_ratio=True,\n",
      "        scale=[\n",
      "            (\n",
      "                1333,\n",
      "                640,\n",
      "            ),\n",
      "            (\n",
      "                1333,\n",
      "                800,\n",
      "            ),\n",
      "        ],\n",
      "        type='RandomResize'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackDetInputs'),\n",
      "]\n",
      "val_ann_file = 'valid/_annotations.coco.json'\n",
      "val_batch_size_per_gpu = 8\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_data_prefix = 'valid/'\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='valid/_annotations.coco.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='valid/'),\n",
      "        data_root='/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file=\n",
      "    '/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1/valid/_annotations.coco.json',\n",
      "    backend_args=None,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "val_interval_stage2 = 1\n",
      "val_num_workers = 3\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "weight_decay = 0.05\n",
      "work_dir = './work_dirs/custom-faster-rcnn_x101-64x4d_fpn_ms-3x_coco'\n",
      "\n",
      "02/25 19:13:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "02/25 19:13:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOWEST      ) EarlyStoppingHook                  \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      "(LOWEST      ) EarlyStoppingHook                  \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "loading annotations into memory...\n",
      "Done (t=0.47s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "02/25 19:14:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - load model from: open-mmlab://resnext101_64x4d\n",
      "02/25 19:14:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by openmmlab backend from path: open-mmlab://resnext101_64x4d\n",
      "Did not find last_checkpoint to be resumed.\n",
      "02/25 19:14:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Auto resumed from the latest checkpoint None.\n",
      "02/25 19:14:02 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "02/25 19:14:02 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "02/25 19:14:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /work/van-speech-nlp/jindaznb/j-vis/mmdetection/work_dirs/custom-faster-rcnn_x101-64x4d_fpn_ms-3x_coco.\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/tools/train.py\", line 121, in <module>\n",
      "    main()\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/tools/train.py\", line 117, in main\n",
      "    runner.train()\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/runner/runner.py\", line 1777, in train\n",
      "    model = self.train_loop.run()  # type: ignore\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/runner/loops.py\", line 96, in run\n",
      "    self.run_epoch()\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/runner/loops.py\", line 112, in run_epoch\n",
      "    self.run_iter(idx, data_batch)\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/runner/loops.py\", line 128, in run_iter\n",
      "    outputs = self.runner.model.train_step(\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py\", line 114, in train_step\n",
      "    losses = self._run_forward(data, mode='loss')  # type: ignore\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py\", line 346, in _run_forward\n",
      "    results = self(**data, mode=mode)\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/detectors/base.py\", line 92, in forward\n",
      "    return self.loss(inputs, data_samples)\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/detectors/two_stage.py\", line 174, in loss\n",
      "    rpn_losses, rpn_results_list = self.rpn_head.loss_and_predict(\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/dense_heads/base_dense_head.py\", line 165, in loss_and_predict\n",
      "    losses = self.loss_by_feat(*loss_inputs)\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/dense_heads/rpn_head.py\", line 125, in loss_by_feat\n",
      "    losses = super().loss_by_feat(\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/dense_heads/anchor_head.py\", line 502, in loss_by_feat\n",
      "    cls_reg_targets = self.get_targets(\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/dense_heads/anchor_head.py\", line 378, in get_targets\n",
      "    results = multi_apply(\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/utils/misc.py\", line 219, in multi_apply\n",
      "    return tuple(map(list, zip(*map_results)))\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/dense_heads/anchor_head.py\", line 252, in _get_targets_single\n",
      "    assign_result = self.assigner.assign(pred_instances, gt_instances,\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/task_modules/assigners/max_iou_assigner.py\", line 220, in assign\n",
      "    overlaps = self.iou_calculator(gt_bboxes_unique, priors)\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/task_modules/assigners/iou2d_calculator.py\", line 62, in __call__\n",
      "    return bbox_overlaps(bboxes1, bboxes2, mode, is_aligned)\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/structures/bbox/bbox_overlaps.py\", line 176, in bbox_overlaps\n",
      "    wh = fp16_clamp(rb - lt, min=0)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 14.58 GiB total capacity; 5.31 GiB already allocated; 62.56 MiB free; 5.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python tools/train.py configs/{model_folder}/{custom_model_name}.py --resume\\\n",
    "    | tee \"../log/OUT_{custom_model_name}_$(date +\"%Y%m%d_%H%M%S\").txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a5ff9a7-82e1-466c-8953-35de0a006b34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m ds \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetectionDataset\u001b[38;5;241m.\u001b[39mfrom_coco(\n\u001b[1;32m      5\u001b[0m     images_directory_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     annotations_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test/_annotations.coco.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ds\u001b[38;5;241m.\u001b[39mimages\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m---> 10\u001b[0m CUSTOM_WEIGHTS_PATH \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/mmdetection/work_dirs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/best_coco_bbox_mAP_50_epoch_*.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m init_detector(CUSTOM_CONFIG_PATH, CUSTOM_WEIGHTS_PATH, device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcallback\u001b[39m(image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetections:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "NMS_IOU_THRESHOLD = 0.5\n",
    "\n",
    "ds = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset_location}/test\",\n",
    "    annotations_path=f\"{dataset_location}/test/_annotations.coco.json\",\n",
    ")\n",
    "\n",
    "images = list(ds.images.values())\n",
    "CUSTOM_WEIGHTS_PATH = glob.glob(f\"{HOME}/mmdetection/work_dirs/{model_name}/best_coco_bbox_mAP_50_epoch_*.pth\")[-1]\n",
    "model = init_detector(CUSTOM_CONFIG_PATH, CUSTOM_WEIGHTS_PATH, device=DEVICE)\n",
    "\n",
    "def callback(image: np.ndarray) -> sv.Detections:\n",
    "    result = inference_detector(model, image)\n",
    "    detections = sv.Detections.from_mmdetection(result)\n",
    "    return detections[detections.confidence > CONFIDENCE_THRESHOLD].with_nms(threshold=NMS_IOU_THRESHOLD)\n",
    "\n",
    "\n",
    "mean_average_precision = sv.MeanAveragePrecision.benchmark(\n",
    "    dataset = ds,\n",
    "    callback = callback\n",
    ")\n",
    "\n",
    "print('mAP:', mean_average_precision.map50_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366c722-c495-4b59-8f86-ba5ded5fbe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset_location}/test\",\n",
    "    annotations_path=f\"{dataset_location}/test/_annotations.coco.json\",\n",
    ")\n",
    "\n",
    "images = list(ds.images.values())\n",
    "CUSTOM_WEIGHTS_PATH = glob.glob(f\"{HOME}/mmdetection/work_dirs/{model_name}/best_coco_bbox_mAP_50_epoch_*.pth\")[-1]\n",
    "model = init_detector(CUSTOM_CONFIG_PATH, CUSTOM_WEIGHTS_PATH, device=DEVICE)\n",
    "\n",
    "best_map, best_params = optimize_params(ds, model, param_space)\n",
    "print('Best mAP:', best_map)\n",
    "print('Best Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52a9aa-b187-4d2b-b64a-067380bc7b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb13ae-9724-473a-a1dd-733d6fdaf8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced3453-1687-4c69-b055-85dfba034281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453232e4-683e-4c41-9975-5639b643af04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
