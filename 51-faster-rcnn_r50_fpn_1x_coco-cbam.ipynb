{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad6e204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvcc: command not found\n",
      "gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "/bin/bash: nvidia-smi: command not found\n",
      "/work/van-speech-nlp/jindaznb/j-vis\n",
      "System Prefix: /work/van-speech-nlp/jindaznb/mmenv\n",
      "HOME: /work/van-speech-nlp/jindaznb/j-vis\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmdet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/ipykernel_7768/1469108342.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_detector, inference_detector\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmdet'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmdet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1-0mmyolo-common.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2432\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2430\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2432\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/IPython/core/magics/execution.py:737\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 737\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_execfile_ipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2954\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[0;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[1;32m   2952\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[0;32m-> 2954\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m   2956\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:270\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/tmp/ipykernel_7768/1469108342.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_detector, inference_detector\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmdet'"
     ]
    }
   ],
   "source": [
    "%run 1-0mmyolo-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c81a8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Write custom Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d024c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME: /work/van-speech-nlp/jindaznb/j-vis\n",
      "/work/van-speech-nlp/jindaznb/j-vis/mmdetection\n"
     ]
    }
   ],
   "source": [
    "# %cd {HOME}/mmyolo\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)\n",
    "%cd {HOME}/mmdetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dfeef84-c3fb-4329-ba12-9b61a5c9d535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/j-vis/mmdetection/configs/faster_rcnn/custom-faster-rcnn_r50_fpn_1x_coco-lab.py\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m      5\u001b[0m dataset_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/work/van-speech-nlp/jindaznb/j-vis/ForestFire2023-5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(CUSTOM_CONFIG_PATH)\n\u001b[1;32m      9\u001b[0m CUSTOM_CONFIG \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m_base_ = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m# ========================Frequently modified parameters======================\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m# -----data related-----\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124mmodel = dict(\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    backbone=dict(\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResNet_CBAM\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m        depth=50,\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m        num_stages=4,\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m        out_indices=(0, 1, 2, 3),\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m        frozen_stages=1,\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m        norm_cfg=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBN\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, requires_grad=True),\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m        norm_eval=True,\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m        style=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m        init_cfg=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPretrained\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, checkpoint=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorchvision://resnet50\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m        use_cbam=True),\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m    neck=dict(\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFPN\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m        in_channels=[256, 512, 1024, 2048],\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m        out_channels=256,\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m        num_outs=5),\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m    )\u001b[39m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124mmodel = dict(\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m    roi_head=dict(\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;124m        bbox_head=dict(num_classes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m))\u001b[39m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124mdataset_type = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOCODataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124mclasses =  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124mdata_root = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m # Root directory of the dataset\u001b[39m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124mtrain_ann_file = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Annotation file for training set\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124mtrain_data_prefix = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Prefix for training data directory\u001b[39m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;124mval_ann_file = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Annotation file for validation set\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124mval_data_prefix = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Prefix for validation data directory\u001b[39m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124mclass_name = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124mnum_classes = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  # Number of classes in the dataset\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124mmetainfo = dict(classes=class_name, palette=[(20, 220, 60)])  # Metadata information for visualization\u001b[39m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;124mtrain_batch_size_per_gpu = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  # Batch size per GPU during training\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124m#train_num_workers = 4  # Number of worker processes for data loading during training\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124mpersistent_workers = True  # Whether to use persistent workers during training\u001b[39m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124m# -----train val related-----\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124mbase_lr = 0.004  # Base learning rate for optimization\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124mmax_epochs = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  # Maximum training epochs\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124mnum_epochs_stage2 = 20  # Number of epochs for stage 2 training\u001b[39m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124mmodel_test_cfg = dict(\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124m    multi_label=True,  # Multi-label configuration for multi-class prediction\u001b[39m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124m    nms_pre=30000,  # Number of boxes before NMS\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124m    score_thr=0.001,  # Score threshold to filter out boxes\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124m    nms=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, iou_threshold=0.65),  # NMS type and threshold\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124m    max_per_img=300)  # Maximum number of detections per image\u001b[39m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;124m# ========================Possible modified parameters========================\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124mdefault_hooks = dict(\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124m    checkpoint=dict(\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpointHook\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124m        save_best=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco/bbox_mAP_50\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124m        rule=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreater\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124m        max_keep_ckpts=10,\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124m    ),\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124m    early_stopping=dict(\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarlyStoppingHook\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124m        monitor=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco/bbox_mAP_50\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124m        patience=20,\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124m        min_delta=0.001\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124m    ),\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124m)\u001b[39m\n\u001b[1;32m     85\u001b[0m \n\u001b[1;32m     86\u001b[0m \u001b[38;5;124mtrain_cfg=dict(\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124m    max_epochs=max_epochs\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124m)\u001b[39m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124mdata = dict(\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124m    samples_per_gpu=8,\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124m    #workers_per_gpu=2,\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124m    train=dict(\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124m        type=dataset_type,\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124m        img_prefix=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124m        classes=classes,\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124m        ann_file=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/_annotations.coco.json.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124m    val=dict(\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124m        type=dataset_type,\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124m        img_prefix=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124m        classes=classes,\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124m        ann_file=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124m    test=dict(\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124m        type=dataset_type,\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124m        img_prefix=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124m        classes=classes,\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124m        ann_file=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m))\u001b[39m\n\u001b[1;32m    108\u001b[0m \n\u001b[1;32m    109\u001b[0m \u001b[38;5;124m# -----data related-----\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124mimg_scale = (1024, 1024)  # width, height\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124m# ratio range for random resize\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124mrandom_resize_ratio_range = (0.1, 2.0)\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124m# Cached images number in mosaic\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124mmosaic_max_cached_images = 40\u001b[39m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124m# Number of cached images in mixupep\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124mmixup_max_cached_images = 20\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124m# Batch size of a single GPU during validation\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124mval_batch_size_per_gpu = 8\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124m# Worker to pre-fetch data for each single GPU during validation\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124mval_num_workers = 3\u001b[39m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;124m# Config of batch shapes. Only on val.\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124mbatch_shapes_cfg = dict(\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124m    type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatchShapePolicy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124m    batch_size=val_batch_size_per_gpu,\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124m    img_size=img_scale[0],\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124m    size_divisor=32,\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124m    extra_pad_ratio=0.5)\u001b[39m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124m# -----train val related-----\u001b[39m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124mlr_start_factor = 1.0e-5\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124mdsl_topk = 13  # Number of bbox selected in each level\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124mloss_cls_weight = 1.0\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124mloss_bbox_weight = 2.0\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124mqfl_beta = 2.0  # beta of QualityFocalLoss\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124mweight_decay = 0.05\u001b[39m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[38;5;124m# Save model checkpoint and validation intervals\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124msave_checkpoint_intervals = 10\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124m# validation intervals in stage 2\u001b[39m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124mval_interval_stage2 = 1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124m# The maximum checkpoints to keep.\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124mmax_keep_ckpts = 3\u001b[39m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124m# single-scale training is recommended to\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124m# be turned on, which can speed up training.\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124menv_cfg = dict(cudnn_benchmark=True)\u001b[39m\n\u001b[1;32m    147\u001b[0m \n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;124mtest_dataloader = dict(\u001b[39m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124m    dataset=dict(\u001b[39m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124m        data_root=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124m    ),)\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124mtest_evaluator = dict(\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124m    ann_file=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/valid/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,)\u001b[39m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124mtrain_dataloader = dict(\u001b[39m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124m    dataset=dict(\u001b[39m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124m        data_root=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124m    ),)\u001b[39m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m \u001b[38;5;124mval_dataloader = dict(\u001b[39m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;124m    dataset=dict(\u001b[39m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[38;5;124m        data_root=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124m    ),)\u001b[39m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124mval_evaluator = dict(\u001b[39m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124m    ann_file=\u001b[39m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/valid/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124m)\u001b[39m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(CUSTOM_CONFIG_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    180\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(CUSTOM_CONFIG)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "model_folder=\"faster_rcnn\"\n",
    "base_model_name=\"faster-rcnn_r50_fpn_1x_coco\"\n",
    "custom_model_name=f\"custom-{base_model_name}-lab\"\n",
    "CUSTOM_CONFIG_PATH = f\"{HOME}/mmdetection/configs/{model_folder}/{custom_model_name}.py\"\n",
    "dataset_location=f\"/work/van-speech-nlp/jindaznb/j-vis/ForestFire2023-5\"\n",
    "print(CUSTOM_CONFIG_PATH)\n",
    "\n",
    "\n",
    "CUSTOM_CONFIG = f\"\"\"\n",
    "_base_ = './{base_model_name}.py'\n",
    "\n",
    "# ========================Frequently modified parameters======================\n",
    "# -----data related-----\n",
    "model = dict(\n",
    "    backbone=dict(\n",
    "        type='ResNet_CBAM',\n",
    "        depth=50,\n",
    "        num_stages=4,\n",
    "        out_indices=(0, 1, 2, 3),\n",
    "        frozen_stages=1,\n",
    "        norm_cfg=dict(type='BN', requires_grad=True),\n",
    "        norm_eval=True,\n",
    "        style='pytorch',\n",
    "        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),\n",
    "        use_cbam=True),\n",
    "    neck=dict(\n",
    "        type='FPN',\n",
    "        in_channels=[256, 512, 1024, 2048],\n",
    "        out_channels=256,\n",
    "        num_outs=5),\n",
    "    )\n",
    "\n",
    "model = dict(\n",
    "    roi_head=dict(\n",
    "        bbox_head=dict(num_classes={num_classes}),\n",
    "))\n",
    "\n",
    "dataset_type = 'COCODataset'\n",
    "classes =  {classes}\n",
    "\n",
    "data_root = '{dataset_location}' # Root directory of the dataset\n",
    "\n",
    "train_ann_file = 'train/_annotations.coco.json'  # Annotation file for training set\n",
    "train_data_prefix = 'train/'  # Prefix for training data directory\n",
    "\n",
    "val_ann_file = 'valid/_annotations.coco.json'  # Annotation file for validation set\n",
    "val_data_prefix = 'valid/'  # Prefix for validation data directory\n",
    "\n",
    "class_name = {classes} \n",
    "num_classes = {num_classes}  # Number of classes in the dataset\n",
    "metainfo = dict(classes=class_name, palette=[(20, 220, 60)])  # Metadata information for visualization\n",
    "\n",
    "train_batch_size_per_gpu = {BATCH_SIZE}  # Batch size per GPU during training\n",
    "#train_num_workers = 4  # Number of worker processes for data loading during training\n",
    "persistent_workers = True  # Whether to use persistent workers during training\n",
    "\n",
    "# -----train val related-----\n",
    "base_lr = 0.004  # Base learning rate for optimization\n",
    "max_epochs = {MAX_EPOCHS}  # Maximum training epochs\n",
    "num_epochs_stage2 = 20  # Number of epochs for stage 2 training\n",
    "\n",
    "model_test_cfg = dict(\n",
    "    multi_label=True,  # Multi-label configuration for multi-class prediction\n",
    "    nms_pre=30000,  # Number of boxes before NMS\n",
    "    score_thr=0.001,  # Score threshold to filter out boxes\n",
    "    nms=dict(type='nms', iou_threshold=0.65),  # NMS type and threshold\n",
    "    max_per_img=300)  # Maximum number of detections per image\n",
    "\n",
    "\n",
    "# ========================Possible modified parameters========================\n",
    "default_hooks = dict(\n",
    "    checkpoint=dict(\n",
    "        type=\"CheckpointHook\",\n",
    "        save_best=\"coco/bbox_mAP_50\",\n",
    "        rule=\"greater\",\n",
    "        max_keep_ckpts=10,\n",
    "    ),\n",
    "    early_stopping=dict(\n",
    "        type=\"EarlyStoppingHook\",\n",
    "        monitor=\"coco/bbox_mAP_50\",\n",
    "        patience=20,\n",
    "        min_delta=0.001\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_cfg=dict(\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "data = dict(\n",
    "    samples_per_gpu=8,\n",
    "    #workers_per_gpu=2,\n",
    "    train=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='train/',\n",
    "        classes=classes,\n",
    "        ann_file='train/_annotations.coco.json.json'),\n",
    "    val=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='valid/',\n",
    "        classes=classes,\n",
    "        ann_file='valid/_annotations.coco.json'),\n",
    "    test=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='test/',\n",
    "        classes=classes,\n",
    "        ann_file='test/_annotations.coco.json'))\n",
    "\n",
    "# -----data related-----\n",
    "img_scale = (1024, 1024)  # width, height\n",
    "# ratio range for random resize\n",
    "random_resize_ratio_range = (0.1, 2.0)\n",
    "# Cached images number in mosaic\n",
    "mosaic_max_cached_images = 40\n",
    "# Number of cached images in mixupep\n",
    "mixup_max_cached_images = 20\n",
    "# Batch size of a single GPU during validation\n",
    "val_batch_size_per_gpu = 8\n",
    "# Worker to pre-fetch data for each single GPU during validation\n",
    "val_num_workers = 3\n",
    "\n",
    "# Config of batch shapes. Only on val.\n",
    "batch_shapes_cfg = dict(\n",
    "    type='BatchShapePolicy',\n",
    "    batch_size=val_batch_size_per_gpu,\n",
    "    img_size=img_scale[0],\n",
    "    size_divisor=32,\n",
    "    extra_pad_ratio=0.5)\n",
    "\n",
    "# -----train val related-----\n",
    "lr_start_factor = 1.0e-5\n",
    "dsl_topk = 13  # Number of bbox selected in each level\n",
    "loss_cls_weight = 1.0\n",
    "loss_bbox_weight = 2.0\n",
    "qfl_beta = 2.0  # beta of QualityFocalLoss\n",
    "weight_decay = 0.05\n",
    "\n",
    "# Save model checkpoint and validation intervals\n",
    "save_checkpoint_intervals = 10\n",
    "# validation intervals in stage 2\n",
    "val_interval_stage2 = 1\n",
    "# The maximum checkpoints to keep.\n",
    "max_keep_ckpts = 3\n",
    "# single-scale training is recommended to\n",
    "# be turned on, which can speed up training.\n",
    "env_cfg = dict(cudnn_benchmark=True)\n",
    "\n",
    "\n",
    "\n",
    "test_dataloader = dict(\n",
    "    dataset=dict(\n",
    "        data_root='{dataset_location}',\n",
    "    ),)\n",
    "test_evaluator = dict(\n",
    "    ann_file='{dataset_location}/valid/_annotations.coco.json',)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = dict(\n",
    "    dataset=dict(\n",
    "        data_root='{dataset_location}',\n",
    "    ),)\n",
    "\n",
    "val_dataloader = dict(\n",
    "    dataset=dict(\n",
    "        data_root='{dataset_location}',\n",
    "    ),)\n",
    "    \n",
    "    \n",
    "val_evaluator = dict(\n",
    "    ann_file=\n",
    "    '{dataset_location}/valid/_annotations.coco.json',\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "with open(CUSTOM_CONFIG_PATH, 'w') as file:\n",
    "    file.write(CUSTOM_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe8b936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python tools/train.py {HOME}/mmdetection/configs/{model_folder}/{custom_model_name}.py --resume\\\n",
    "    | tee \"../log/OUT_{base_model_name}_$(date +\"%Y%m%d_%H%M%S\").txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a5ff9a7-82e1-466c-8953-35de0a006b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /work/van-speech-nlp/jindaznb/j-vis/mmdetection/work_dirs/custom-faster-rcnn_r50_fpn_1x_coco-cbam/best_coco_bbox_mAP_50_epoch_298.pth\n",
      "mAP: 0.32640031258336616\n"
     ]
    }
   ],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "NMS_IOU_THRESHOLD = 0.5\n",
    "\n",
    "ds = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset_location}/test\",\n",
    "    annotations_path=f\"{dataset_location}/test/_annotations.coco.json\",\n",
    ")\n",
    "\n",
    "images = list(ds.images.values())\n",
    "CUSTOM_WEIGHTS_PATH = glob.glob(f\"{HOME}/mmdetection/work_dirs/{custom_model_name}/best_coco_bbox_mAP_50_epoch_*.pth\")[0]\n",
    "model = init_detector(CUSTOM_CONFIG_PATH, CUSTOM_WEIGHTS_PATH, device=DEVICE)\n",
    "\n",
    "def callback(image: np.ndarray) -> sv.Detections:\n",
    "    result = inference_detector(model, image)\n",
    "    detections = sv.Detections.from_mmdetection(result)\n",
    "    return detections[detections.confidence > CONFIDENCE_THRESHOLD].with_nms(threshold=NMS_IOU_THRESHOLD)\n",
    "\n",
    "\n",
    "mean_average_precision = sv.MeanAveragePrecision.benchmark(\n",
    "    dataset = ds,\n",
    "    callback = callback\n",
    ")\n",
    "\n",
    "print('mAP:', mean_average_precision.map50_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0366c722-c495-4b59-8f86-ba5ded5fbe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /work/van-speech-nlp/jindaznb/j-vis/mmdetection/work_dirs/custom-faster-rcnn_r50_fpn_1x_coco-cbam/best_coco_bbox_mAP_50_epoch_298.pth\n",
      "|   iter    |  target   | confid... | nms_io... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.3219   \u001b[0m | \u001b[0m0.4371   \u001b[0m | \u001b[0m0.9556   \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.3502   \u001b[0m | \u001b[95m0.7588   \u001b[0m | \u001b[95m0.6388   \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.3366   \u001b[0m | \u001b[0m0.2404   \u001b[0m | \u001b[0m0.2404   \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.3097   \u001b[0m | \u001b[0m0.1523   \u001b[0m | \u001b[0m0.8796   \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.3367   \u001b[0m | \u001b[0m0.641    \u001b[0m | \u001b[0m0.7373   \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.3865   \u001b[0m | \u001b[95m0.8994   \u001b[0m | \u001b[95m0.3813   \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.3576   \u001b[0m | \u001b[0m0.7639   \u001b[0m | \u001b[0m0.4472   \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.566    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.3696   \u001b[0m | \u001b[0m0.8055   \u001b[0m | \u001b[0m0.3326   \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.3336   \u001b[0m | \u001b[0m0.6025   \u001b[0m | \u001b[0m0.551    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.3262   \u001b[0m | \u001b[0m0.4976   \u001b[0m | \u001b[0m0.8419   \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.3439   \u001b[0m | \u001b[0m0.4921   \u001b[0m | \u001b[0m0.2995   \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.3836   \u001b[0m | \u001b[0m0.8926   \u001b[0m | \u001b[0m0.3936   \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.3145   \u001b[0m | \u001b[0m0.2725   \u001b[0m | \u001b[0m0.5231   \u001b[0m |\n",
      "| \u001b[95m16       \u001b[0m | \u001b[95m0.3865   \u001b[0m | \u001b[95m0.8984   \u001b[0m | \u001b[95m0.383    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.3532   \u001b[0m | \u001b[0m0.7733   \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.3484   \u001b[0m | \u001b[0m0.4336   \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.3374   \u001b[0m | \u001b[0m0.1012   \u001b[0m | \u001b[0m0.1013   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.3349   \u001b[0m | \u001b[0m0.6196   \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.3224   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m0.4007   \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.3626   \u001b[0m | \u001b[0m0.8373   \u001b[0m | \u001b[0m0.7127   \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.3078   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m0.6572   \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.3586   \u001b[0m | \u001b[0m0.6294   \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.3499   \u001b[0m | \u001b[0m0.7535   \u001b[0m | \u001b[0m0.8464   \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.3177   \u001b[0m | \u001b[0m0.3366   \u001b[0m | \u001b[0m0.7264   \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.3543   \u001b[0m | \u001b[0m0.645    \u001b[0m | \u001b[0m0.2763   \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.3138   \u001b[0m | \u001b[0m0.2583   \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.3263   \u001b[0m | \u001b[0m0.4437   \u001b[0m | \u001b[0m0.4715   \u001b[0m |\n",
      "=================================================\n",
      "Best mAP: 0.38653258669753315\n",
      "Best Parameters: [0.898384540775208, 0.38299213942809107]\n"
     ]
    }
   ],
   "source": [
    "ds = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset_location}/test\",\n",
    "    annotations_path=f\"{dataset_location}/test/_annotations.coco.json\",\n",
    ")\n",
    "\n",
    "images = list(ds.images.values())\n",
    "CUSTOM_WEIGHTS_PATH = glob.glob(f\"{HOME}/mmdetection/work_dirs/{custom_model_name}/best_coco_bbox_mAP_50_epoch_*.pth\")[0]\n",
    "model = init_detector(CUSTOM_CONFIG_PATH, CUSTOM_WEIGHTS_PATH, device=DEVICE)\n",
    "\n",
    "best_map, best_params = optimize_params(ds, model, param_space)\n",
    "print('Best mAP:', best_map)\n",
    "print('Best Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52a9aa-b187-4d2b-b64a-067380bc7b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8feb13ae-9724-473a-a1dd-733d6fdaf8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced3453-1687-4c69-b055-85dfba034281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453232e4-683e-4c41-9975-5639b643af04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
