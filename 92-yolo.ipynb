{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad6e204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvcc: command not found\n",
      "gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "/bin/bash: nvidia-smi: command not found\n",
      "/work/van-speech-nlp/jindaznb/j-vis\n",
      "System Prefix: /work/van-speech-nlp/jindaznb/visenv\n",
      "HOME: /work/van-speech-nlp/jindaznb/j-vis\n",
      "2.0.0+cu117 False\n",
      "3.3.0\n",
      "11.7\n",
      "GCC 9.3\n"
     ]
    }
   ],
   "source": [
    "%run 1-0mmyolo-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c81a8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Write custom Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d024c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/j-vis/mmyolo\n"
     ]
    }
   ],
   "source": [
    "# %cd {HOME}/mmyolo\n",
    "%cd {HOME}/mmyolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a837c9-00f9-4d0a-9ae6-735d3264a606",
   "metadata": {},
   "source": [
    "# base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e2991d8-51c3-4d23-935e-40c050f32c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/j-vis/mmyolo/configs/yolov8/custom-yolov8_s_syncbn_fast_8xb16-500e_coco-2024.py\n"
     ]
    }
   ],
   "source": [
    "model_folder=\"yolov8\"\n",
    "base_model_name=\"yolov8_s_syncbn_fast_8xb16-500e_coco\"\n",
    "custom_model_name=f\"custom-{base_model_name}-2024\"\n",
    "CUSTOM_CONFIG_PATH = f\"{HOME}/mmyolo/configs/{model_folder}/{custom_model_name}.py\"\n",
    "MAX_EPOCHS=300\n",
    "print(CUSTOM_CONFIG_PATH)\n",
    "dataset_location=dataset_location=f\"/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1\"\n",
    "\n",
    "CUSTOM_CONFIG = f\"\"\"\n",
    "_base_ = './{base_model_name}.py'\n",
    "\n",
    "# ========================Frequently modified parameters======================\n",
    "# -----data related-----\n",
    "\n",
    "\n",
    "dataset_type = 'COCODataset'\n",
    "classes =  {classes}\n",
    "\n",
    "data_root = '{dataset_location}' # Root directory of the dataset\n",
    "\n",
    "train_ann_file = 'train/_annotations.coco.json'  # Annotation file for training set\n",
    "train_data_prefix = 'train/'  # Prefix for training data directory\n",
    "\n",
    "val_ann_file = 'valid/_annotations.coco.json'  # Annotation file for validation set\n",
    "val_data_prefix = 'valid/'  # Prefix for validation data directory\n",
    "\n",
    "class_name = {classes} \n",
    "num_classes = {num_classes}  # Number of classes in the dataset\n",
    "metainfo = dict(classes=class_name, palette=[(20, 220, 60)])  # Metadata information for visualization\n",
    "\n",
    "persistent_workers = True  # Whether to use persistent workers during training\n",
    "\n",
    "# -----train val related-----\n",
    "# base_lr = 0.004  # Base learning rate for optimization\n",
    "base_lr = 0.04\n",
    "max_epochs = {MAX_EPOCHS}  # Maximum training epochs\n",
    "num_epochs_stage2 = 20  # Number of epochs for stage 2 training\n",
    "\n",
    "\n",
    "\n",
    "model_test_cfg = dict(\n",
    "    multi_label=True,  # Multi-label configuration for multi-class prediction\n",
    "    nms_pre=30000,  # Number of boxes before NMS\n",
    "    score_thr=0.001,  # Score threshold to filter out boxes\n",
    "    nms=dict(type='nms', iou_threshold=0.65),  # NMS type and threshold\n",
    "    max_per_img=300)  # Maximum number of detections per image\n",
    "\n",
    "\n",
    "# ========================Possible modified parameters========================\n",
    "default_hooks = dict(\n",
    "    checkpoint=dict(\n",
    "        type=\"CheckpointHook\",\n",
    "        save_best=\"coco/bbox_mAP_50\",\n",
    "        rule=\"greater\",\n",
    "        max_keep_ckpts=10,\n",
    "    ),\n",
    "    early_stopping=dict(\n",
    "        type=\"EarlyStoppingHook\",\n",
    "        monitor=\"coco/bbox_mAP_50\",\n",
    "        patience=25,\n",
    "        min_delta=0.001\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_cfg=dict(\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "data = dict(\n",
    "    samples_per_gpu=1,\n",
    "    #workers_per_gpu=2,\n",
    "    train=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='train/',\n",
    "        classes=classes,\n",
    "        ann_file='train/_annotations.coco.json'),\n",
    "    val=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='valid/',\n",
    "        classes=classes,\n",
    "        ann_file='valid/_annotations.coco.json'),\n",
    "    test=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='test/',\n",
    "        classes=classes,\n",
    "        ann_file='test/_annotations.coco.json'))\n",
    "\n",
    "# -----data related-----\n",
    "img_scale = (1024, 1024)  # width, height\n",
    "# ratio range for random resize\n",
    "random_resize_ratio_range = (0.1, 2.0)\n",
    "# Cached images number in mosaic\n",
    "mosaic_max_cached_images = 40\n",
    "# Number of cached images in mixupep\n",
    "mixup_max_cached_images = 20\n",
    "# Batch size of a single GPU during validation\n",
    "val_batch_size_per_gpu = 8\n",
    "# Worker to pre-fetch data for each single GPU during validation\n",
    "val_num_workers = 3\n",
    "\n",
    "\"\"\"\n",
    "with open(CUSTOM_CONFIG_PATH, 'w') as file:\n",
    "    file.write(CUSTOM_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc2a1e-e00f-44da-9132-1d6fce9a9837",
   "metadata": {},
   "source": [
    "###### log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "859749f1-9f95-4a4b-892d-8d696c95adcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/j-vis/mmdetection/configs/yolov8/custom-yolov8_l_syncbn_fast_8xb16-500e_coco-log-2024.py\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid format specifier",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(CUSTOM_CONFIG_PATH)\n\u001b[1;32m      7\u001b[0m dataset_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m CUSTOM_CONFIG \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m_base_ = [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../_base_/default_runtime.py\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../_base_/det_p5_tta.py\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m# ========================Frequently modified parameters======================\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m# -----data related-----\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m# model = dict(\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m#     roi_head=dict(\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m#         bbox_head=dict(num_classes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m# ))\u001b[39m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124mdataset_type = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOCODataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mclasses =  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;124mdata_root = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m # Root directory of the dataset\u001b[39m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;124mtrain_ann_file = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Annotation file for training set\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124mtrain_data_prefix = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Prefix for training data directory\u001b[39m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124mval_ann_file = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Annotation file for validation set\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124mval_data_prefix = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Prefix for validation data directory\u001b[39m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124mclass_name = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124mnum_classes = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  # Number of classes in the dataset\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124mmetainfo = dict(classes=class_name, palette=[(20, 220, 60)])  # Metadata information for visualization\u001b[39m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m# -----train val related-----\u001b[39m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m# -----train val related-----\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m# Base learning rate for optim_wrapper. Corresponding to 8xb16=64 bs\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124mbase_lr = 0.01\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124mmax_epochs = 500  # Maximum training epochs\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124m# Disable mosaic augmentation for final 10 epochs (stage 2)\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124mclose_mosaic_epochs = 10\u001b[39m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124mmodel_test_cfg = dict(\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124m    # The config of multi-label for multi-class prediction.\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124m    multi_label=True,\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124m    # The number of boxes before NMS\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m    nms_pre=30000,\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124m    score_thr=0.001,  # Threshold to filter out boxes.\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124m    nms=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, iou_threshold=0.7),  # NMS type and threshold\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124m    max_per_img=300)  # Max number of detections of each image\u001b[39m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;124m# ========================Possible modified parameters========================\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124m# -----data related-----\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124mimg_scale = (640, 640)  # width, height\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124m# Dataset type, this will be used to define the dataset\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124mdataset_type = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv5CocoDataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124m# Batch size of a single GPU during validation\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124mval_batch_size_per_gpu = 1\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124m# Worker to pre-fetch data for each single GPU during validation\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124mval_num_workers = 2\u001b[39m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124m# Config of batch shapes. Only on val.\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124m# We tested YOLOv8-m will get 0.02 higher than not using it.\u001b[39m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124mbatch_shapes_cfg = None\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124m# You can turn on `batch_shapes_cfg` by uncommenting the following lines.\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124m# batch_shapes_cfg = dict(\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124m#     type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatchShapePolicy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124m#     batch_size=val_batch_size_per_gpu,\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124m#     img_size=img_scale[0],\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124m#     # The image scale of padding should be divided by pad_size_divisor\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m#     size_divisor=32,\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124m#     # Additional paddings for pixel scale\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124m#     extra_pad_ratio=0.5)\u001b[39m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124m# -----model related-----\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124m# The scaling factor that controls the depth of the network structure\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124mdeepen_factor = 0.33\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124m# The scaling factor that controls the width of the network structure\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124mwiden_factor = 0.5\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124m# Strides of multi-scale prior box\u001b[39m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124mstrides = [8, 16, 32]\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124m# The output channel of the last stage\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124mlast_stage_out_channels = 1024\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124mnum_det_layers = 3  # The number of model output scales\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124mnorm_cfg = dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBN\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, momentum=0.03, eps=0.001)  # Normalization config\u001b[39m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;124m# -----train val related-----\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124maffine_scale = 0.5  # YOLOv5RandomAffine scaling ratio\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124m# YOLOv5RandomAffine aspect ratio of width and height thres to filter bboxes\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124mmax_aspect_ratio = 100\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124mtal_topk = 10  # Number of bbox selected in each level\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124mtal_alpha = 0.5  # A Hyper-parameter related to alignment_metrics\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124mtal_beta = 6.0  # A Hyper-parameter related to alignment_metrics\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124m# TODO: Automatically scale loss_weight based on number of detection layers\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124mloss_cls_weight = 0.5\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124mloss_bbox_weight = 7.5\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124m# Since the dfloss is implemented differently in the official\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124m# and mmdet, we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre going to divide loss_weight by 4.\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124mloss_dfl_weight = 1.5 / 4\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124mlr_factor = 0.01  # Learning rate scaling factor\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124mweight_decay = 0.0005\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124m# Save model checkpoint and validation intervals in stage 1\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124msave_epoch_intervals = 10\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124m# validation intervals in stage 2\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124mval_interval_stage2 = 1\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124m# The maximum checkpoints to keep.\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124mmax_keep_ckpts = 2\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124m# Single-scale training is recommended to\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124m# be turned on, which can speed up training.\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124menv_cfg = dict(cudnn_benchmark=True)\u001b[39m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;124m# ===============================Unmodified in most cases====================\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124mmodel = dict(\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124m    type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLODetector\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124m    data_preprocessor=dict(\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv5DetDataPreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124m        mean=[0., 0., 0.],\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124m        std=[255., 255., 255.],\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124m        bgr_to_rgb=True),\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124m    backbone=dict(\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv8CSPDarknet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124m        arch=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124m        last_stage_out_channels=last_stage_out_channels,\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124m        deepen_factor=deepen_factor,\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124m        widen_factor=widen_factor,\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124m        norm_cfg=norm_cfg,\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124m        act_cfg=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSiLU\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, inplace=True)),\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124m    neck=dict(\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv8PAFPN\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124m        deepen_factor=deepen_factor,\u001b[39m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124m        widen_factor=widen_factor,\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124m        in_channels=[256, 512, last_stage_out_channels],\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124m        out_channels=[256, 512, last_stage_out_channels],\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124m        num_csp_blocks=3,\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124m        norm_cfg=norm_cfg,\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124m        act_cfg=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSiLU\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, inplace=True)),\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124m    bbox_head=dict(\u001b[39m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv8Head\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124m        head_module=dict(\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124m            type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv8HeadModule\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124m            num_classes=num_classes,\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124m            in_channels=[256, 512, last_stage_out_channels],\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124m            widen_factor=widen_factor,\u001b[39m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124m            reg_max=16,\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124m            norm_cfg=norm_cfg,\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124m            act_cfg=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSiLU\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, inplace=True),\u001b[39m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124m            featmap_strides=strides),\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124m        prior_generator=dict(\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124m            type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmdet.MlvlPointGenerator\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, offset=0.5, strides=strides),\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124m        bbox_coder=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistancePointBBoxCoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124m        # scaled based on number of detection layers\u001b[39m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124m        loss_cls=dict(\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124m            type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmdet.CrossEntropyLoss\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124m            use_sigmoid=True,\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124m            reduction=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124m            loss_weight=loss_cls_weight),\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124m        loss_bbox=dict(\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124m            type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIoULoss\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124m            iou_mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mciou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124m            bbox_format=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxyxy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124m            reduction=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124m            loss_weight=loss_bbox_weight,\u001b[39m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124m            return_iou=False),\u001b[39m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124m        loss_dfl=dict(\u001b[39m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124m            type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmdet.DistributionFocalLoss\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124m            reduction=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124m            loss_weight=loss_dfl_weight)),\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124m    train_cfg=dict(\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124m        assigner=dict(\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124m            type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatchTaskAlignedAssigner\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124m            num_classes=num_classes,\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124m            use_ciou=True,\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124m            topk=tal_topk,\u001b[39m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124m            alpha=tal_alpha,\u001b[39m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124m            beta=tal_beta,\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124m            eps=1e-9)),\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124m    test_cfg=model_test_cfg)\u001b[39m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124malbu_train_transforms = [\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlur\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, p=0.01),\u001b[39m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedianBlur\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, p=0.01),\u001b[39m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToGray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, p=0.01),\u001b[39m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLAHE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, p=0.01)\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124m]\u001b[39m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m \u001b[38;5;124mpre_transform = [\u001b[39m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoadImageFromFile\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, backend_args=_base_.backend_args),\u001b[39m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoadAnnotations\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, with_bbox=True)\u001b[39m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124m]\u001b[39m\n\u001b[1;32m    190\u001b[0m \n\u001b[1;32m    191\u001b[0m \u001b[38;5;124mlast_transform = [\u001b[39m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124m    dict(\u001b[39m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmdet.Albu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124m        transforms=albu_train_transforms,\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124m        bbox_params=dict(\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124m            type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBboxParams\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124m            format=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpascal_voc\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124m            label_fields=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgt_bboxes_labels\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgt_ignore_flags\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]),\u001b[39m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124m        keymap=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgt_bboxes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv5HSVRandomAug\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmdet.RandomFlip\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, prob=0.5),\u001b[39m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124m    dict(\u001b[39m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmdet.PackDetInputs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124m        meta_keys=(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_id\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mori_shape\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_shape\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflip\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124m                   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflip_direction\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m))\u001b[39m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124m]\u001b[39m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[38;5;124mtrain_pipeline = [\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124m    *pre_transform,\u001b[39m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124m    dict(\u001b[39m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMosaic\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124m        img_scale=img_scale,\u001b[39m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124m        pad_val=114.0,\u001b[39m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124m        pre_transform=pre_transform),\u001b[39m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124m    dict(\u001b[39m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv5RandomAffine\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124m        max_rotate_degree=0.0,\u001b[39m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124m        max_shear_degree=0.0,\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124m        scaling_ratio_range=(1 - affine_scale, 1 + affine_scale),\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124m        max_aspect_ratio=max_aspect_ratio,\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124m        # img_scale is (width, height)\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124m        border=(-img_scale[0] // 2, -img_scale[1] // 2),\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124m        border_val=(114, 114, 114)),\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124m    *last_transform\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124m]\u001b[39m\n\u001b[1;32m    229\u001b[0m \n\u001b[1;32m    230\u001b[0m \u001b[38;5;124mtrain_pipeline_stage2 = [\u001b[39m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124m    *pre_transform,\u001b[39m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv5KeepRatioResize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, scale=img_scale),\u001b[39m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124m    dict(\u001b[39m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLetterResize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124m        scale=img_scale,\u001b[39m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124m        allow_scale_up=True,\u001b[39m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124m        pad_val=dict(img=114.0)),\u001b[39m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124m    dict(\u001b[39m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv5RandomAffine\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124m        max_rotate_degree=0.0,\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124m        max_shear_degree=0.0,\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124m        scaling_ratio_range=(1 - affine_scale, 1 + affine_scale),\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124m        max_aspect_ratio=max_aspect_ratio,\u001b[39m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124m        border_val=(114, 114, 114)), *last_transform\u001b[39m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124m]\u001b[39m\n\u001b[1;32m    246\u001b[0m \n\u001b[1;32m    247\u001b[0m \u001b[38;5;124mtrain_dataloader = dict(\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124m    batch_size=train_batch_size_per_gpu,\u001b[39m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124m    num_workers=train_num_workers,\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124m    persistent_workers=persistent_workers,\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124m    pin_memory=True,\u001b[39m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124m    sampler=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDefaultSampler\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, shuffle=True),\u001b[39m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124m    collate_fn=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov5_collate\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124m    dataset=dict(\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124m        type=dataset_type,\u001b[39m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124m        data_root=data_root,\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124m        ann_file=train_ann_file,\u001b[39m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124m        data_prefix=dict(img=train_data_prefix),\u001b[39m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124m        filter_cfg=dict(filter_empty_gt=False, min_size=32),\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124m        pipeline=train_pipeline))\u001b[39m\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m \u001b[38;5;124mtest_pipeline = [\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoadImageFromFile\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, backend_args=_base_.backend_args),\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv5KeepRatioResize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, scale=img_scale),\u001b[39m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124m    dict(\u001b[39m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLetterResize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124m        scale=img_scale,\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124m        allow_scale_up=False,\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124m        pad_val=dict(img=114)),\u001b[39m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124m    dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoadAnnotations\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, with_bbox=True, _scope_=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmdet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124m    dict(\u001b[39m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmdet.PackDetInputs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124m        meta_keys=(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_id\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mori_shape\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_shape\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124m                   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_factor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_param\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m))\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124m]\u001b[39m\n\u001b[1;32m    276\u001b[0m \n\u001b[1;32m    277\u001b[0m \u001b[38;5;124mval_dataloader = dict(\u001b[39m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124m    batch_size=val_batch_size_per_gpu,\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124m    num_workers=val_num_workers,\u001b[39m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124m    persistent_workers=persistent_workers,\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124m    pin_memory=True,\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124m    drop_last=False,\u001b[39m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124m    sampler=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDefaultSampler\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, shuffle=False),\u001b[39m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124m    dataset=dict(\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124m        type=dataset_type,\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124m        data_root=data_root,\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124m        test_mode=True,\u001b[39m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124m        data_prefix=dict(img=val_data_prefix),\u001b[39m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124m        ann_file=val_ann_file,\u001b[39m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124m        pipeline=test_pipeline,\u001b[39m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124m        batch_shapes_cfg=batch_shapes_cfg))\u001b[39m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124mtest_dataloader = val_dataloader\u001b[39m\n\u001b[1;32m    294\u001b[0m \n\u001b[1;32m    295\u001b[0m \u001b[38;5;124mparam_scheduler = None\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124moptim_wrapper = dict(\u001b[39m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124m    type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimWrapper\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124m    clip_grad=dict(max_norm=10.0),\u001b[39m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124m    optimizer=dict(\u001b[39m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124m        lr=base_lr,\u001b[39m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124m        momentum=0.937,\u001b[39m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124m        weight_decay=weight_decay,\u001b[39m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124m        nesterov=True,\u001b[39m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124m        batch_size_per_gpu=train_batch_size_per_gpu),\u001b[39m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124m    constructor=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv5OptimizerConstructor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124mdefault_hooks = dict(\u001b[39m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124m    param_scheduler=dict(\u001b[39m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv5ParamSchedulerHook\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124m        scheduler_type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124m        lr_factor=lr_factor,\u001b[39m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124m        max_epochs=max_epochs),\u001b[39m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124m    checkpoint=dict(\u001b[39m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCheckpointHook\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124m        interval=save_epoch_intervals,\u001b[39m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124m        save_best=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124m        max_keep_ckpts=max_keep_ckpts))\u001b[39m\n\u001b[1;32m    319\u001b[0m \n\u001b[1;32m    320\u001b[0m \u001b[38;5;124mcustom_hooks = [\u001b[39m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124m    dict(\u001b[39m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEMAHook\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124m        ema_type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpMomentumEMA\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124m        momentum=0.0001,\u001b[39m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124m        update_buffers=True,\u001b[39m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124m        strict_load=False,\u001b[39m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124m        priority=49),\u001b[39m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124m    dict(\u001b[39m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmdet.PipelineSwitchHook\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124m        switch_epoch=max_epochs - close_mosaic_epochs,\u001b[39m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124m        switch_pipeline=train_pipeline_stage2)\u001b[39m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124m]\u001b[39m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[38;5;124mval_evaluator = dict(\u001b[39m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124m    type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmdet.CocoMetric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124m    proposal_nums=(100, 1, 10),\u001b[39m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124m    ann_file=data_root + val_ann_file,\u001b[39m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124m    metric=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124mtest_evaluator = val_evaluator\u001b[39m\n\u001b[1;32m    340\u001b[0m \n\u001b[1;32m    341\u001b[0m \u001b[38;5;124mtrain_cfg = dict(\u001b[39m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124m    type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochBasedTrainLoop\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124m    max_epochs=max_epochs,\u001b[39m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124m    val_interval=save_epoch_intervals,\u001b[39m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124m    dynamic_intervals=[((max_epochs - close_mosaic_epochs),\u001b[39m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124m                        val_interval_stage2)])\u001b[39m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124mval_cfg = dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValLoop\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124mtest_cfg = dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTestLoop\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(CUSTOM_CONFIG_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    353\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(CUSTOM_CONFIG)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid format specifier"
     ]
    }
   ],
   "source": [
    "model_folder=\"yolov8\"\n",
    "base_model_name=\"yolov8_l_syncbn_fast_8xb16-500e_coco\"\n",
    "custom_model_name=f\"custom-{base_model_name}-log-2024\"\n",
    "CUSTOM_CONFIG_PATH = f\"{HOME}/mmdetection/con figs/{model_folder}/{custom_model_name}.py\"\n",
    "MAX_EPOCHS=300\n",
    "print(CUSTOM_CONFIG_PATH)\n",
    "dataset_location=f\"/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1\"\n",
    "\n",
    "CUSTOM_CONFIG = f\"\"\"\n",
    "_base_ = ['../_base_/default_runtime.py', '../_base_/det_p5_tta.py']\n",
    "\n",
    "# ========================Frequently modified parameters======================\n",
    "# -----data related-----\n",
    "# model = dict(\n",
    "#     roi_head=dict(\n",
    "#         bbox_head=dict(num_classes={num_classes}),\n",
    "# ))\n",
    "\n",
    "dataset_type = 'COCODataset'\n",
    "classes =  {classes}\n",
    "\n",
    "data_root = '{dataset_location}' # Root directory of the dataset\n",
    "\n",
    "train_ann_file = 'train/_annotations.coco.json'  # Annotation file for training set\n",
    "train_data_prefix = 'train/'  # Prefix for training data directory\n",
    "\n",
    "val_ann_file = 'valid/_annotations.coco.json'  # Annotation file for validation set\n",
    "val_data_prefix = 'valid/'  # Prefix for validation data directory\n",
    "\n",
    "class_name = {classes} \n",
    "num_classes = {num_classes}  # Number of classes in the dataset\n",
    "metainfo = dict(classes=class_name, palette=[(20, 220, 60)])  # Metadata information for visualization\n",
    "\n",
    "# -----train val related-----\n",
    "\n",
    "# -----train val related-----\n",
    "# Base learning rate for optim_wrapper. Corresponding to 8xb16=64 bs\n",
    "base_lr = 0.01\n",
    "max_epochs = 500  # Maximum training epochs\n",
    "# Disable mosaic augmentation for final 10 epochs (stage 2)\n",
    "close_mosaic_epochs = 10\n",
    "\n",
    "model_test_cfg = dict(\n",
    "    # The config of multi-label for multi-class prediction.\n",
    "    multi_label=True,\n",
    "    # The number of boxes before NMS\n",
    "    nms_pre=30000,\n",
    "    score_thr=0.001,  # Threshold to filter out boxes.\n",
    "    nms=dict(type='nms', iou_threshold=0.7),  # NMS type and threshold\n",
    "    max_per_img=300)  # Max number of detections of each image\n",
    "\n",
    "# ========================Possible modified parameters========================\n",
    "# -----data related-----\n",
    "img_scale = (640, 640)  # width, height\n",
    "# Dataset type, this will be used to define the dataset\n",
    "dataset_type = 'YOLOv5CocoDataset'\n",
    "# Batch size of a single GPU during validation\n",
    "val_batch_size_per_gpu = 1\n",
    "# Worker to pre-fetch data for each single GPU during validation\n",
    "val_num_workers = 2\n",
    "\n",
    "# Config of batch shapes. Only on val.\n",
    "# We tested YOLOv8-m will get 0.02 higher than not using it.\n",
    "batch_shapes_cfg = None\n",
    "# You can turn on `batch_shapes_cfg` by uncommenting the following lines.\n",
    "# batch_shapes_cfg = dict(\n",
    "#     type='BatchShapePolicy',\n",
    "#     batch_size=val_batch_size_per_gpu,\n",
    "#     img_size=img_scale[0],\n",
    "#     # The image scale of padding should be divided by pad_size_divisor\n",
    "#     size_divisor=32,\n",
    "#     # Additional paddings for pixel scale\n",
    "#     extra_pad_ratio=0.5)\n",
    "\n",
    "# -----model related-----\n",
    "# The scaling factor that controls the depth of the network structure\n",
    "deepen_factor = 0.33\n",
    "# The scaling factor that controls the width of the network structure\n",
    "widen_factor = 0.5\n",
    "# Strides of multi-scale prior box\n",
    "strides = [8, 16, 32]\n",
    "# The output channel of the last stage\n",
    "last_stage_out_channels = 1024\n",
    "num_det_layers = 3  # The number of model output scales\n",
    "norm_cfg = dict(type='BN', momentum=0.03, eps=0.001)  # Normalization config\n",
    "\n",
    "# -----train val related-----\n",
    "affine_scale = 0.5  # YOLOv5RandomAffine scaling ratio\n",
    "# YOLOv5RandomAffine aspect ratio of width and height thres to filter bboxes\n",
    "max_aspect_ratio = 100\n",
    "tal_topk = 10  # Number of bbox selected in each level\n",
    "tal_alpha = 0.5  # A Hyper-parameter related to alignment_metrics\n",
    "tal_beta = 6.0  # A Hyper-parameter related to alignment_metrics\n",
    "# TODO: Automatically scale loss_weight based on number of detection layers\n",
    "loss_cls_weight = 0.5\n",
    "loss_bbox_weight = 7.5\n",
    "# Since the dfloss is implemented differently in the official\n",
    "# and mmdet, we're going to divide loss_weight by 4.\n",
    "loss_dfl_weight = 1.5 / 4\n",
    "lr_factor = 0.01  # Learning rate scaling factor\n",
    "weight_decay = 0.0005\n",
    "# Save model checkpoint and validation intervals in stage 1\n",
    "save_epoch_intervals = 10\n",
    "# validation intervals in stage 2\n",
    "val_interval_stage2 = 1\n",
    "# The maximum checkpoints to keep.\n",
    "max_keep_ckpts = 2\n",
    "# Single-scale training is recommended to\n",
    "# be turned on, which can speed up training.\n",
    "env_cfg = dict(cudnn_benchmark=True)\n",
    "\n",
    "# ===============================Unmodified in most cases====================\n",
    "model = dict(\n",
    "    type='YOLODetector',\n",
    "    data_preprocessor=dict(\n",
    "        type='YOLOv5DetDataPreprocessor',\n",
    "        mean=[0., 0., 0.],\n",
    "        std=[255., 255., 255.],\n",
    "        bgr_to_rgb=True),\n",
    "    backbone=dict(\n",
    "        type='YOLOv8CSPDarknet',\n",
    "        arch='P5',\n",
    "        last_stage_out_channels=last_stage_out_channels,\n",
    "        deepen_factor=deepen_factor,\n",
    "        widen_factor=widen_factor,\n",
    "        norm_cfg=norm_cfg,\n",
    "        act_cfg=dict(type='SiLU', inplace=True)),\n",
    "    neck=dict(\n",
    "        type='YOLOv8PAFPN',\n",
    "        deepen_factor=deepen_factor,\n",
    "        widen_factor=widen_factor,\n",
    "        in_channels=[256, 512, last_stage_out_channels],\n",
    "        out_channels=[256, 512, last_stage_out_channels],\n",
    "        num_csp_blocks=3,\n",
    "        norm_cfg=norm_cfg,\n",
    "        act_cfg=dict(type='SiLU', inplace=True)),\n",
    "    bbox_head=dict(\n",
    "        type='YOLOv8Head',\n",
    "        head_module=dict(\n",
    "            type='YOLOv8HeadModule',\n",
    "            num_classes=num_classes,\n",
    "            in_channels=[256, 512, last_stage_out_channels],\n",
    "            widen_factor=widen_factor,\n",
    "            reg_max=16,\n",
    "            norm_cfg=norm_cfg,\n",
    "            act_cfg=dict(type='SiLU', inplace=True),\n",
    "            featmap_strides=strides),\n",
    "        prior_generator=dict(\n",
    "            type='mmdet.MlvlPointGenerator', offset=0.5, strides=strides),\n",
    "        bbox_coder=dict(type='DistancePointBBoxCoder'),\n",
    "        # scaled based on number of detection layers\n",
    "        loss_cls=dict(\n",
    "            type='mmdet.CrossEntropyLoss',\n",
    "            use_sigmoid=True,\n",
    "            reduction='none',\n",
    "            loss_weight=loss_cls_weight),\n",
    "        loss_bbox=dict(\n",
    "            type='IoULoss',\n",
    "            iou_mode='ciou',\n",
    "            bbox_format='xyxy',\n",
    "            reduction='sum',\n",
    "            loss_weight=loss_bbox_weight,\n",
    "            return_iou=False),\n",
    "        loss_dfl=dict(\n",
    "            type='mmdet.DistributionFocalLoss',\n",
    "            reduction='mean',\n",
    "            loss_weight=loss_dfl_weight)),\n",
    "    train_cfg=dict(\n",
    "        assigner=dict(\n",
    "            type='BatchTaskAlignedAssigner',\n",
    "            num_classes=num_classes,\n",
    "            use_ciou=True,\n",
    "            topk=tal_topk,\n",
    "            alpha=tal_alpha,\n",
    "            beta=tal_beta,\n",
    "            eps=1e-9)),\n",
    "    test_cfg=model_test_cfg)\n",
    "\n",
    "albu_train_transforms = [\n",
    "    dict(type='Blur', p=0.01),\n",
    "    dict(type='MedianBlur', p=0.01),\n",
    "    dict(type='ToGray', p=0.01),\n",
    "    dict(type='CLAHE', p=0.01)\n",
    "]\n",
    "\n",
    "pre_transform = [\n",
    "    dict(type='LoadImageFromFile', backend_args=_base_.backend_args),\n",
    "    dict(type='LoadAnnotations', with_bbox=True)\n",
    "]\n",
    "\n",
    "last_transform = [\n",
    "    dict(\n",
    "        type='mmdet.Albu',\n",
    "        transforms=albu_train_transforms,\n",
    "        bbox_params=dict(\n",
    "            type='BboxParams',\n",
    "            format='pascal_voc',\n",
    "            label_fields=['gt_bboxes_labels', 'gt_ignore_flags']),\n",
    "        keymap={\n",
    "            'img': 'image',\n",
    "            'gt_bboxes': 'bboxes'\n",
    "        }),\n",
    "    dict(type='YOLOv5HSVRandomAug'),\n",
    "    dict(type='mmdet.RandomFlip', prob=0.5),\n",
    "    dict(\n",
    "        type='mmdet.PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'flip',\n",
    "                   'flip_direction'))\n",
    "]\n",
    "\n",
    "train_pipeline = [\n",
    "    *pre_transform,\n",
    "    dict(\n",
    "        type='Mosaic',\n",
    "        img_scale=img_scale,\n",
    "        pad_val=114.0,\n",
    "        pre_transform=pre_transform),\n",
    "    dict(\n",
    "        type='YOLOv5RandomAffine',\n",
    "        max_rotate_degree=0.0,\n",
    "        max_shear_degree=0.0,\n",
    "        scaling_ratio_range=(1 - affine_scale, 1 + affine_scale),\n",
    "        max_aspect_ratio=max_aspect_ratio,\n",
    "        # img_scale is (width, height)\n",
    "        border=(-img_scale[0] // 2, -img_scale[1] // 2),\n",
    "        border_val=(114, 114, 114)),\n",
    "    *last_transform\n",
    "]\n",
    "\n",
    "train_pipeline_stage2 = [\n",
    "    *pre_transform,\n",
    "    dict(type='YOLOv5KeepRatioResize', scale=img_scale),\n",
    "    dict(\n",
    "        type='LetterResize',\n",
    "        scale=img_scale,\n",
    "        allow_scale_up=True,\n",
    "        pad_val=dict(img=114.0)),\n",
    "    dict(\n",
    "        type='YOLOv5RandomAffine',\n",
    "        max_rotate_degree=0.0,\n",
    "        max_shear_degree=0.0,\n",
    "        scaling_ratio_range=(1 - affine_scale, 1 + affine_scale),\n",
    "        max_aspect_ratio=max_aspect_ratio,\n",
    "        border_val=(114, 114, 114)), *last_transform\n",
    "]\n",
    "\n",
    "train_dataloader = dict(\n",
    "    batch_size=train_batch_size_per_gpu,\n",
    "    num_workers=train_num_workers,\n",
    "    persistent_workers=persistent_workers,\n",
    "    pin_memory=True,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
    "    collate_fn=dict(type='yolov5_collate'),\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        data_root=data_root,\n",
    "        ann_file=train_ann_file,\n",
    "        data_prefix=dict(img=train_data_prefix),\n",
    "        filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
    "        pipeline=train_pipeline))\n",
    "\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=_base_.backend_args),\n",
    "    dict(type='YOLOv5KeepRatioResize', scale=img_scale),\n",
    "    dict(\n",
    "        type='LetterResize',\n",
    "        scale=img_scale,\n",
    "        allow_scale_up=False,\n",
    "        pad_val=dict(img=114)),\n",
    "    dict(type='LoadAnnotations', with_bbox=True, _scope_='mmdet'),\n",
    "    dict(\n",
    "        type='mmdet.PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
    "                   'scale_factor', 'pad_param'))\n",
    "]\n",
    "\n",
    "val_dataloader = dict(\n",
    "    batch_size=val_batch_size_per_gpu,\n",
    "    num_workers=val_num_workers,\n",
    "    persistent_workers=persistent_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        data_root=data_root,\n",
    "        test_mode=True,\n",
    "        data_prefix=dict(img=val_data_prefix),\n",
    "        ann_file=val_ann_file,\n",
    "        pipeline=test_pipeline,\n",
    "        batch_shapes_cfg=batch_shapes_cfg))\n",
    "\n",
    "test_dataloader = val_dataloader\n",
    "\n",
    "param_scheduler = None\n",
    "optim_wrapper = dict(\n",
    "    type='OptimWrapper',\n",
    "    clip_grad=dict(max_norm=10.0),\n",
    "    optimizer=dict(\n",
    "        type='SGD',\n",
    "        lr=base_lr,\n",
    "        momentum=0.937,\n",
    "        weight_decay=weight_decay,\n",
    "        nesterov=True,\n",
    "        batch_size_per_gpu=train_batch_size_per_gpu),\n",
    "    constructor='YOLOv5OptimizerConstructor')\n",
    "\n",
    "default_hooks = dict(\n",
    "    param_scheduler=dict(\n",
    "        type='YOLOv5ParamSchedulerHook',\n",
    "        scheduler_type='linear',\n",
    "        lr_factor=lr_factor,\n",
    "        max_epochs=max_epochs),\n",
    "    checkpoint=dict(\n",
    "        type='CheckpointHook',\n",
    "        interval=save_epoch_intervals,\n",
    "        save_best='auto',\n",
    "        max_keep_ckpts=max_keep_ckpts))\n",
    "\n",
    "custom_hooks = [\n",
    "    dict(\n",
    "        type='EMAHook',\n",
    "        ema_type='ExpMomentumEMA',\n",
    "        momentum=0.0001,\n",
    "        update_buffers=True,\n",
    "        strict_load=False,\n",
    "        priority=49),\n",
    "    dict(\n",
    "        type='mmdet.PipelineSwitchHook',\n",
    "        switch_epoch=max_epochs - close_mosaic_epochs,\n",
    "        switch_pipeline=train_pipeline_stage2)\n",
    "]\n",
    "\n",
    "val_evaluator = dict(\n",
    "    type='mmdet.CocoMetric',\n",
    "    proposal_nums=(100, 1, 10),\n",
    "    ann_file=data_root + val_ann_file,\n",
    "    metric='bbox')\n",
    "test_evaluator = val_evaluator\n",
    "\n",
    "train_cfg = dict(\n",
    "    type='EpochBasedTrainLoop',\n",
    "    max_epochs=max_epochs,\n",
    "    val_interval=save_epoch_intervals,\n",
    "    dynamic_intervals=[((max_epochs - close_mosaic_epochs),\n",
    "                        val_interval_stage2)])\n",
    "\n",
    "val_cfg = dict(type='ValLoop')\n",
    "test_cfg = dict(type='TestLoop')\n",
    "\n",
    "\"\"\"\n",
    "with open(CUSTOM_CONFIG_PATH, 'w') as file:\n",
    "    file.write(CUSTOM_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be5807-2e75-4321-ba2d-c47fe8b10522",
   "metadata": {},
   "source": [
    "# multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfeef84-c3fb-4329-ba12-9b61a5c9d535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_folder=\"yolov8\"\n",
    "base_model_name=\"yolov8_l_syncbn_fast_8xb16-500e_coco\"\n",
    "custom_model_name=f\"custom-{base_model_name}-multi-2024\"\n",
    "CUSTOM_CONFIG_PATH = f\"{HOME}/mmdetection/configs/{model_folder}/{custom_model_name}.py\"\n",
    "MAX_EPOCHS=300\n",
    "print(CUSTOM_CONFIG_PATH)\n",
    "dataset_location=f\"/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1\"\n",
    "\n",
    "CUSTOM_CONFIG = f\"\"\"\n",
    "_base_ = './{base_model_name}.py'\n",
    "\n",
    "# ========================Frequently modified parameters======================\n",
    "# -----data related-----\n",
    "# model = dict(\n",
    "#     roi_head=dict(\n",
    "#         bbox_head=dict(num_classes={num_classes}),\n",
    "# ))\n",
    "\n",
    "dataset_type = 'COCODataset'\n",
    "classes =  {classes}\n",
    "\n",
    "data_root = '{dataset_location}' # Root directory of the dataset\n",
    "\n",
    "train_ann_file = 'train/_annotations.coco.json'  # Annotation file for training set\n",
    "train_data_prefix = 'train/'  # Prefix for training data directory\n",
    "\n",
    "val_ann_file = 'valid/_annotations.coco.json'  # Annotation file for validation set\n",
    "val_data_prefix = 'valid/'  # Prefix for validation data directory\n",
    "\n",
    "class_name = {classes} \n",
    "num_classes = {num_classes}  # Number of classes in the dataset\n",
    "metainfo = dict(classes=class_name, palette=[(20, 220, 60)])  # Metadata information for visualization\n",
    "\n",
    "train_batch_size_per_gpu = {BATCH_SIZE}  # Batch size per GPU during training\n",
    "#train_num_workers = 4  # Number of worker processes for data loading during training\n",
    "persistent_workers = True  # Whether to use persistent workers during training\n",
    "\n",
    "# -----train val related-----\n",
    "# base_lr = 0.004  # Base learning rate for optimization\n",
    "base_lr = 0.04\n",
    "max_epochs = {MAX_EPOCHS}  # Maximum training epochs\n",
    "num_epochs_stage2 = 20  # Number of epochs for stage 2 training\n",
    "\n",
    "model_test_cfg = dict(\n",
    "    multi_label=True,  # Multi-label configuration for multi-class prediction\n",
    "    nms_pre=30000,  # Number of boxes before NMS\n",
    "    score_thr=0.001,  # Score threshold to filter out boxes\n",
    "    nms=dict(type='nms', iou_threshold=0.65),  # NMS type and threshold\n",
    "    max_per_img=300)  # Maximum number of detections per image\n",
    "\n",
    "\n",
    "# ========================Possible modified parameters========================\n",
    "default_hooks = dict(\n",
    "    checkpoint=dict(\n",
    "        type=\"CheckpointHook\",\n",
    "        save_best=\"coco/bbox_mAP_50\",\n",
    "        rule=\"greater\",\n",
    "        max_keep_ckpts=10,\n",
    "    ),\n",
    "    early_stopping=dict(\n",
    "        type=\"EarlyStoppingHook\",\n",
    "        monitor=\"coco/bbox_mAP_50\",\n",
    "        patience=25,\n",
    "        min_delta=0.001\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_cfg=dict(\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "data = dict(\n",
    "    samples_per_gpu=1,\n",
    "    #workers_per_gpu=2,\n",
    "    train=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='train/',\n",
    "        classes=classes,\n",
    "        img_scale=[(1333, 800), (1666, 1000)], # mutiscale\n",
    "        ann_file='train/_annotations.coco.json'),\n",
    "    val=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='valid/',\n",
    "        classes=classes,\n",
    "        ann_file='valid/_annotations.coco.json'),\n",
    "    test=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='test/',\n",
    "        classes=classes,\n",
    "        ann_file='test/_annotations.coco.json'))\n",
    "\n",
    "# -----data related-----\n",
    "img_scale = (1024, 1024)  # width, height\n",
    "# ratio range for random resize\n",
    "random_resize_ratio_range = (0.1, 2.0)\n",
    "# Cached images number in mosaic\n",
    "mosaic_max_cached_images = 40\n",
    "# Number of cached images in mixupep\n",
    "mixup_max_cached_images = 20\n",
    "# Batch size of a single GPU during validation\n",
    "val_batch_size_per_gpu = 8\n",
    "# Worker to pre-fetch data for each single GPU during validation\n",
    "val_num_workers = 3\n",
    "\n",
    "\n",
    "# -----train val related-----\n",
    "lr_start_factor = 1.0e-5\n",
    "dsl_topk = 13  # Number of bbox selected in each level\n",
    "loss_cls_weight = 1.0\n",
    "loss_bbox_weight = 2.0\n",
    "qfl_beta = 2.0  # beta of QualityFocalLoss\n",
    "weight_decay = 0.05\n",
    "\n",
    "# Save model checkpoint and validation intervals\n",
    "save_checkpoint_intervals = 10\n",
    "# validation intervals in stage 2\n",
    "val_interval_stage2 = 1\n",
    "# The maximum checkpoints to keep.\n",
    "max_keep_ckpts = 3\n",
    "# single-scale training is recommended to\n",
    "# be turned on, which can speed up training.\n",
    "env_cfg = dict(cudnn_benchmark=True)\n",
    "\"\"\"\n",
    "with open(CUSTOM_CONFIG_PATH, 'w') as file:\n",
    "    file.write(CUSTOM_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5812340-da00-478b-a831-b354a56eed91",
   "metadata": {},
   "source": [
    "# log + multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215808f-5b01-4d24-aa58-f198344f88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder=\"yolov8\"\n",
    "base_model_name=\"yolov8_l_syncbn_fast_8xb16-500e_coco\"\n",
    "custom_model_name=f\"custom-{base_model_name}-log-multi-2024\"\n",
    "CUSTOM_CONFIG_PATH = f\"{HOME}/mmdetection/configs/{model_folder}/{custom_model_name}.py\"\n",
    "MAX_EPOCHS=300\n",
    "print(CUSTOM_CONFIG_PATH)\n",
    "dataset_location=f\"/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1-log\"\n",
    "\n",
    "CUSTOM_CONFIG = f\"\"\"\n",
    "_base_ = './{base_model_name}.py'\n",
    "\n",
    "# ========================Frequently modified parameters======================\n",
    "# -----data related-----\n",
    "# model = dict(\n",
    "#     roi_head=dict(\n",
    "#         bbox_head=dict(num_classes={num_classes}),\n",
    "# ))\n",
    "\n",
    "dataset_type = 'COCODataset'\n",
    "classes =  {classes}\n",
    "\n",
    "data_root = '{dataset_location}' # Root directory of the dataset\n",
    "\n",
    "train_ann_file = 'train/_annotations.coco.json'  # Annotation file for training set\n",
    "train_data_prefix = 'train/'  # Prefix for training data directory\n",
    "\n",
    "val_ann_file = 'valid/_annotations.coco.json'  # Annotation file for validation set\n",
    "val_data_prefix = 'valid/'  # Prefix for validation data directory\n",
    "\n",
    "class_name = {classes} \n",
    "num_classes = {num_classes}  # Number of classes in the dataset\n",
    "metainfo = dict(classes=class_name, palette=[(20, 220, 60)])  # Metadata information for visualization\n",
    "\n",
    "train_batch_size_per_gpu = {BATCH_SIZE}  # Batch size per GPU during training\n",
    "#train_num_workers = 4  # Number of worker processes for data loading during training\n",
    "persistent_workers = True  # Whether to use persistent workers during training\n",
    "\n",
    "# -----train val related-----\n",
    "# base_lr = 0.004  # Base learning rate for optimization\n",
    "base_lr = 0.04\n",
    "max_epochs = {MAX_EPOCHS}  # Maximum training epochs\n",
    "num_epochs_stage2 = 20  # Number of epochs for stage 2 training\n",
    "\n",
    "model_test_cfg = dict(\n",
    "    multi_label=True,  # Multi-label configuration for multi-class prediction\n",
    "    nms_pre=30000,  # Number of boxes before NMS\n",
    "    score_thr=0.001,  # Score threshold to filter out boxes\n",
    "    nms=dict(type='nms', iou_threshold=0.65),  # NMS type and threshold\n",
    "    max_per_img=300)  # Maximum number of detections per image\n",
    "\n",
    "\n",
    "# ========================Possible modified parameters========================\n",
    "default_hooks = dict(\n",
    "    checkpoint=dict(\n",
    "        type=\"CheckpointHook\",\n",
    "        save_best=\"coco/bbox_mAP_50\",\n",
    "        rule=\"greater\",\n",
    "        max_keep_ckpts=10,\n",
    "    ),\n",
    "    early_stopping=dict(\n",
    "        type=\"EarlyStoppingHook\",\n",
    "        monitor=\"coco/bbox_mAP_50\",\n",
    "        patience=25,\n",
    "        min_delta=0.001\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_cfg=dict(\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "data = dict(\n",
    "    samples_per_gpu=1,\n",
    "    #workers_per_gpu=2,\n",
    "    train=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='train/',\n",
    "        classes=classes,\n",
    "        img_scale=[(1333, 800), (1666, 1000)], # mutiscale\n",
    "        ann_file='train/_annotations.coco.json'),\n",
    "    val=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='valid/',\n",
    "        classes=classes,\n",
    "        ann_file='valid/_annotations.coco.json'),\n",
    "    test=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='test/',\n",
    "        classes=classes,\n",
    "        ann_file='test/_annotations.coco.json'))\n",
    "\n",
    "# -----data related-----\n",
    "img_scale = (1024, 1024)  # width, height\n",
    "# ratio range for random resize\n",
    "random_resize_ratio_range = (0.1, 2.0)\n",
    "# Cached images number in mosaic\n",
    "mosaic_max_cached_images = 40\n",
    "# Number of cached images in mixupep\n",
    "mixup_max_cached_images = 20\n",
    "# Batch size of a single GPU during validation\n",
    "val_batch_size_per_gpu = 8\n",
    "# Worker to pre-fetch data for each single GPU during validation\n",
    "val_num_workers = 3\n",
    "\n",
    "# -----train val related-----\n",
    "lr_start_factor = 1.0e-5\n",
    "dsl_topk = 13  # Number of bbox selected in each level\n",
    "loss_cls_weight = 1.0\n",
    "loss_bbox_weight = 2.0\n",
    "qfl_beta = 2.0  # beta of QualityFocalLoss\n",
    "weight_decay = 0.05\n",
    "\n",
    "# Save model checkpoint and validation intervals\n",
    "save_checkpoint_intervals = 10\n",
    "# validation intervals in stage 2\n",
    "val_interval_stage2 = 1\n",
    "# The maximum checkpoints to keep.\n",
    "max_keep_ckpts = 3\n",
    "# single-scale training is recommended to\n",
    "# be turned on, which can speed up training.\n",
    "env_cfg = dict(cudnn_benchmark=True)\n",
    "\"\"\"\n",
    "with open(CUSTOM_CONFIG_PATH, 'w') as file:\n",
    "    file.write(CUSTOM_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8b936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python tools/train.py configs/{model_folder}/{custom_model_name}.py --resume\\\n",
    "    | tee \"../log/OUT_{custom_model_name}_$(date +\"%Y%m%d_%H%M%S\").txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ff9a7-82e1-466c-8953-35de0a006b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "NMS_IOU_THRESHOLD = 0.5\n",
    "\n",
    "ds = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset_location}/test\",\n",
    "    annotations_path=f\"{dataset_location}/test/_annotations.coco.json\",\n",
    ")\n",
    "\n",
    "images = list(ds.images.values())\n",
    "CUSTOM_WEIGHTS_PATH = glob.glob(f\"{HOME}/mmdetection/work_dirs/{custom_model_name}/best_coco_bbox_mAP_50_epoch_*.pth\")[-1]\n",
    "model = init_detector(CUSTOM_CONFIG_PATH, CUSTOM_WEIGHTS_PATH, device=DEVICE)\n",
    "\n",
    "def callback(image: np.ndarray) -> sv.Detections:\n",
    "    result = inference_detector(model, image)\n",
    "    detections = sv.Detections.from_mmdetection(result)\n",
    "    return detections[detections.confidence > CONFIDENCE_THRESHOLD].with_nms(threshold=NMS_IOU_THRESHOLD)\n",
    "\n",
    "\n",
    "mean_average_precision = sv.MeanAveragePrecision.benchmark(\n",
    "    dataset = ds,\n",
    "    callback = callback\n",
    ")\n",
    "\n",
    "print('mAP:', mean_average_precision.map50_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366c722-c495-4b59-8f86-ba5ded5fbe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset_location}/test\",\n",
    "    annotations_path=f\"{dataset_location}/test/_annotations.coco.json\",\n",
    ")\n",
    "\n",
    "images = list(ds.images.values())\n",
    "CUSTOM_WEIGHTS_PATH = glob.glob(f\"{HOME}/mmdetection/work_dirs/{custom_model_name}/best_coco_bbox_mAP_50_epoch_*.pth\")[-1]\n",
    "model = init_detector(CUSTOM_CONFIG_PATH, CUSTOM_WEIGHTS_PATH, device=DEVICE)\n",
    "\n",
    "best_map, best_params = optimize_params(ds, model, param_space)\n",
    "print('Best mAP:', best_map)\n",
    "print('Best Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52a9aa-b187-4d2b-b64a-067380bc7b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb13ae-9724-473a-a1dd-733d6fdaf8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced3453-1687-4c69-b055-85dfba034281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453232e4-683e-4c41-9975-5639b643af04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
