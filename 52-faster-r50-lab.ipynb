{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad6e204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvcc: command not found\n",
      "gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "/bin/bash: nvidia-smi: command not found\n",
      "/work/van-speech-nlp/jindaznb/j-vis\n",
      "System Prefix: /work/van-speech-nlp/jenv\n",
      "HOME: /work/van-speech-nlp/jindaznb/j-vis\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/ipykernel_15294/722178902.py:21\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_detector, inference_detector\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) OpenMMLab. All rights reserved.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmmcv\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmmengine\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m digit_version\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmcv/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) OpenMMLab. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marraymisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmcv/image/__init__.py:10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (cutout, imcrop, imflip, imflip_, impad,\n\u001b[1;32m      6\u001b[0m                         impad_to_multiple, imrescale, imresize, imresize_like,\n\u001b[1;32m      7\u001b[0m                         imresize_to_multiple, imrotate, imshear, imtranslate,\n\u001b[1;32m      8\u001b[0m                         rescale_size)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imfrombytes, imread, imwrite, supported_backends, use_backend\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor2imgs\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mphotometric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (adjust_brightness, adjust_color, adjust_contrast,\n\u001b[1;32m     12\u001b[0m                           adjust_hue, adjust_lighting, adjust_sharpness,\n\u001b[1;32m     13\u001b[0m                           auto_contrast, clahe, imdenormalize, imequalize,\n\u001b[1;32m     14\u001b[0m                           iminvert, imnormalize, imnormalize_, lut_transform,\n\u001b[1;32m     15\u001b[0m                           posterize, solarize)\n\u001b[1;32m     17\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbgr2gray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbgr2hls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbgr2hsv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbgr2rgb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray2bgr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray2rgb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhls2bgr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhsv2bgr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimconvert\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb2bgr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb2gray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimrescale\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madjust_hue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     29\u001b[0m ]\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmcv/image/misc.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmmcv\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     torch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/torch/__init__.py:457\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    459\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1-0mmyolo-common.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2432\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2430\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2432\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/IPython/core/magics/execution.py:737\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 737\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_execfile_ipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2954\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[0;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[1;32m   2952\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[0;32m-> 2954\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m   2956\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:270\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/tmp/ipykernel_15294/722178902.py:21\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_detector, inference_detector\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) OpenMMLab. All rights reserved.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmmcv\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmmengine\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m digit_version\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmcv/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) OpenMMLab. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marraymisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmcv/image/__init__.py:10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (cutout, imcrop, imflip, imflip_, impad,\n\u001b[1;32m      6\u001b[0m                         impad_to_multiple, imrescale, imresize, imresize_like,\n\u001b[1;32m      7\u001b[0m                         imresize_to_multiple, imrotate, imshear, imtranslate,\n\u001b[1;32m      8\u001b[0m                         rescale_size)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imfrombytes, imread, imwrite, supported_backends, use_backend\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor2imgs\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mphotometric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (adjust_brightness, adjust_color, adjust_contrast,\n\u001b[1;32m     12\u001b[0m                           adjust_hue, adjust_lighting, adjust_sharpness,\n\u001b[1;32m     13\u001b[0m                           auto_contrast, clahe, imdenormalize, imequalize,\n\u001b[1;32m     14\u001b[0m                           iminvert, imnormalize, imnormalize_, lut_transform,\n\u001b[1;32m     15\u001b[0m                           posterize, solarize)\n\u001b[1;32m     17\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbgr2gray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbgr2hls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbgr2hsv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbgr2rgb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray2bgr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray2rgb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhls2bgr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhsv2bgr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimconvert\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb2bgr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb2gray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimrescale\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madjust_hue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     29\u001b[0m ]\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmcv/image/misc.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmmcv\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     torch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/torch/__init__.py:457\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    459\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "%run 1-0mmyolo-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c81a8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Write custom Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d024c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/j-vis/mmdetection\n"
     ]
    }
   ],
   "source": [
    "# %cd {HOME}/mmyolo\n",
    "%cd {HOME}/mmdetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dfeef84-c3fb-4329-ba12-9b61a5c9d535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/j-vis/mmdetection/configs/faster_rcnn/custom-faster-rcnn_r50_fpn_1x_coco-lab.py\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m      5\u001b[0m dataset_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1-lab\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(CUSTOM_CONFIG_PATH)\n\u001b[1;32m      9\u001b[0m CUSTOM_CONFIG \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m_base_ = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m# ========================Frequently modified parameters======================\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m# -----data related-----\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124mmodel = dict(\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    roi_head=dict(\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;124m        bbox_head=dict(num_classes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m))\u001b[39m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124mdataset_type = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOCODataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mclasses =  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;124mdata_root = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m # Root directory of the dataset\u001b[39m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;124mtrain_ann_file = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Annotation file for training set\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124mtrain_data_prefix = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Prefix for training data directory\u001b[39m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124mval_ann_file = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Annotation file for validation set\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124mval_data_prefix = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  # Prefix for validation data directory\u001b[39m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124mclass_name = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124mnum_classes = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  # Number of classes in the dataset\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124mmetainfo = dict(classes=class_name, palette=[(20, 220, 60)])  # Metadata information for visualization\u001b[39m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;124mtrain_batch_size_per_gpu = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  # Batch size per GPU during training\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124m#train_num_workers = 4  # Number of worker processes for data loading during training\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124mpersistent_workers = True  # Whether to use persistent workers during training\u001b[39m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m# -----train val related-----\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124mbase_lr = 0.004  # Base learning rate for optimization\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124mmax_epochs = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  # Maximum training epochs\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124mnum_epochs_stage2 = 20  # Number of epochs for stage 2 training\u001b[39m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124mmodel_test_cfg = dict(\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124m    multi_label=True,  # Multi-label configuration for multi-class prediction\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124m    nms_pre=30000,  # Number of boxes before NMS\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124m    score_thr=0.001,  # Score threshold to filter out boxes\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m    nms=dict(type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, iou_threshold=0.65),  # NMS type and threshold\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124m    max_per_img=300)  # Maximum number of detections per image\u001b[39m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;124m# ========================Possible modified parameters========================\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124mdefault_hooks = dict(\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124m    checkpoint=dict(\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpointHook\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124m        save_best=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco/bbox_mAP_50\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124m        rule=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreater\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124m        max_keep_ckpts=10,\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124m    ),\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124m    early_stopping=dict(\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124m        type=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarlyStoppingHook\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124m        monitor=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco/bbox_mAP_50\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124m        patience=20,\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124m        min_delta=0.001\u001b[39m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124m    ),\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124m)\u001b[39m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124mtrain_cfg=dict(\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124m    max_epochs=max_epochs\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124m)\u001b[39m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124mdata = dict(\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124m    samples_per_gpu=8,\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124m    #workers_per_gpu=2,\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124m    train=dict(\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124m        type=dataset_type,\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124m        img_prefix=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124m        classes=classes,\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124m        ann_file=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/_annotations.coco.json.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124m    val=dict(\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124m        type=dataset_type,\u001b[39m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124m        img_prefix=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124m        classes=classes,\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124m        ann_file=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m),\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124m    test=dict(\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124m        type=dataset_type,\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124m        img_prefix=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124m        classes=classes,\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124m        ann_file=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m))\u001b[39m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124m# -----data related-----\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124mimg_scale = (1024, 1024)  # width, height\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124m# ratio range for random resize\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124mrandom_resize_ratio_range = (0.1, 2.0)\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124m# Cached images number in mosaic\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124mmosaic_max_cached_images = 40\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124m# Number of cached images in mixupep\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124mmixup_max_cached_images = 20\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124m# Batch size of a single GPU during validation\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124mval_batch_size_per_gpu = 8\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124m# Worker to pre-fetch data for each single GPU during validation\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124mval_num_workers = 3\u001b[39m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124m# Config of batch shapes. Only on val.\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124mbatch_shapes_cfg = dict(\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124m    type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatchShapePolicy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124m    batch_size=val_batch_size_per_gpu,\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124m    img_size=img_scale[0],\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124m    size_divisor=32,\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124m    extra_pad_ratio=0.5)\u001b[39m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124m# -----train val related-----\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124mlr_start_factor = 1.0e-5\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124mdsl_topk = 13  # Number of bbox selected in each level\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124mloss_cls_weight = 1.0\u001b[39m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124mloss_bbox_weight = 2.0\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124mqfl_beta = 2.0  # beta of QualityFocalLoss\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124mweight_decay = 0.05\u001b[39m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124m# Save model checkpoint and validation intervals\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124msave_checkpoint_intervals = 10\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124m# validation intervals in stage 2\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124mval_interval_stage2 = 1\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124m# The maximum checkpoints to keep.\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124mmax_keep_ckpts = 3\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124m# single-scale training is recommended to\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124m# be turned on, which can speed up training.\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124menv_cfg = dict(cudnn_benchmark=True)\u001b[39m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124mtest_dataloader = dict(\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124m    dataset=dict(\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124m        data_root=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124m    ),)\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124mtest_evaluator = dict(\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124m    ann_file=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/valid/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,)\u001b[39m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124mtrain_dataloader = dict(\u001b[39m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;124m    dataset=dict(\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124m        data_root=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124m    ),)\u001b[39m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[38;5;124mval_dataloader = dict(\u001b[39m\n\u001b[1;32m    147\u001b[0m \n\u001b[1;32m    148\u001b[0m \u001b[38;5;124m    dataset=dict(\u001b[39m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;124m        data_root=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124m    ),)\u001b[39m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124mval_evaluator = dict(\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124m    ann_file=\u001b[39m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/valid/_annotations.coco.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124m)\u001b[39m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(CUSTOM_CONFIG_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    161\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(CUSTOM_CONFIG)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "model_folder=\"faster_rcnn\"\n",
    "base_model_name=\"faster-rcnn_r50_fpn_1x_coco\"\n",
    "custom_model_name=f\"custom-{base_model_name}-lab\"\n",
    "CUSTOM_CONFIG_PATH = f\"{HOME}/mmdetection/configs/{model_folder}/{custom_model_name}.py\"\n",
    "dataset_location=f\"/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1-lab\"\n",
    "print(CUSTOM_CONFIG_PATH)\n",
    "\n",
    "\n",
    "CUSTOM_CONFIG = f\"\"\"\n",
    "_base_ = './{base_model_name}.py'\n",
    "\n",
    "# ========================Frequently modified parameters======================\n",
    "# -----data related-----\n",
    "model = dict(\n",
    "    roi_head=dict(\n",
    "        bbox_head=dict(num_classes={num_classes}),\n",
    "))\n",
    "\n",
    "dataset_type = 'COCODataset'\n",
    "classes =  {classes}\n",
    "\n",
    "data_root = '{dataset_location}' # Root directory of the dataset\n",
    "\n",
    "train_ann_file = 'train/_annotations.coco.json'  # Annotation file for training set\n",
    "train_data_prefix = 'train/'  # Prefix for training data directory\n",
    "\n",
    "val_ann_file = 'valid/_annotations.coco.json'  # Annotation file for validation set\n",
    "val_data_prefix = 'valid/'  # Prefix for validation data directory\n",
    "\n",
    "class_name = {classes} \n",
    "num_classes = {num_classes}  # Number of classes in the dataset\n",
    "metainfo = dict(classes=class_name, palette=[(20, 220, 60)])  # Metadata information for visualization\n",
    "\n",
    "train_batch_size_per_gpu = {BATCH_SIZE}  # Batch size per GPU during training\n",
    "#train_num_workers = 4  # Number of worker processes for data loading during training\n",
    "persistent_workers = True  # Whether to use persistent workers during training\n",
    "\n",
    "# -----train val related-----\n",
    "base_lr = 0.004  # Base learning rate for optimization\n",
    "max_epochs = {MAX_EPOCHS}  # Maximum training epochs\n",
    "num_epochs_stage2 = 20  # Number of epochs for stage 2 training\n",
    "\n",
    "model_test_cfg = dict(\n",
    "    multi_label=True,  # Multi-label configuration for multi-class prediction\n",
    "    nms_pre=30000,  # Number of boxes before NMS\n",
    "    score_thr=0.001,  # Score threshold to filter out boxes\n",
    "    nms=dict(type='nms', iou_threshold=0.65),  # NMS type and threshold\n",
    "    max_per_img=300)  # Maximum number of detections per image\n",
    "\n",
    "\n",
    "# ========================Possible modified parameters========================\n",
    "default_hooks = dict(\n",
    "    checkpoint=dict(\n",
    "        type=\"CheckpointHook\",\n",
    "        save_best=\"coco/bbox_mAP_50\",\n",
    "        rule=\"greater\",\n",
    "        max_keep_ckpts=10,\n",
    "    ),\n",
    "    early_stopping=dict(\n",
    "        type=\"EarlyStoppingHook\",\n",
    "        monitor=\"coco/bbox_mAP_50\",\n",
    "        patience=20,\n",
    "        min_delta=0.001\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_cfg=dict(\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "data = dict(\n",
    "    samples_per_gpu=8,\n",
    "    #workers_per_gpu=2,\n",
    "    train=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='train/',\n",
    "        classes=classes,\n",
    "        ann_file='train/_annotations.coco.json.json'),\n",
    "    val=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='valid/',\n",
    "        classes=classes,\n",
    "        ann_file='valid/_annotations.coco.json'),\n",
    "    test=dict(\n",
    "        type=dataset_type,\n",
    "        img_prefix='test/',\n",
    "        classes=classes,\n",
    "        ann_file='test/_annotations.coco.json'))\n",
    "\n",
    "# -----data related-----\n",
    "img_scale = (1024, 1024)  # width, height\n",
    "# ratio range for random resize\n",
    "random_resize_ratio_range = (0.1, 2.0)\n",
    "# Cached images number in mosaic\n",
    "mosaic_max_cached_images = 40\n",
    "# Number of cached images in mixupep\n",
    "mixup_max_cached_images = 20\n",
    "# Batch size of a single GPU during validation\n",
    "val_batch_size_per_gpu = 8\n",
    "# Worker to pre-fetch data for each single GPU during validation\n",
    "val_num_workers = 3\n",
    "\n",
    "# Config of batch shapes. Only on val.\n",
    "batch_shapes_cfg = dict(\n",
    "    type='BatchShapePolicy',\n",
    "    batch_size=val_batch_size_per_gpu,\n",
    "    img_size=img_scale[0],\n",
    "    size_divisor=32,\n",
    "    extra_pad_ratio=0.5)\n",
    "\n",
    "# -----train val related-----\n",
    "lr_start_factor = 1.0e-5\n",
    "dsl_topk = 13  # Number of bbox selected in each level\n",
    "loss_cls_weight = 1.0\n",
    "loss_bbox_weight = 2.0\n",
    "qfl_beta = 2.0  # beta of QualityFocalLoss\n",
    "weight_decay = 0.05\n",
    "\n",
    "# Save model checkpoint and validation intervals\n",
    "save_checkpoint_intervals = 10\n",
    "# validation intervals in stage 2\n",
    "val_interval_stage2 = 1\n",
    "# The maximum checkpoints to keep.\n",
    "max_keep_ckpts = 3\n",
    "# single-scale training is recommended to\n",
    "# be turned on, which can speed up training.\n",
    "env_cfg = dict(cudnn_benchmark=True)\n",
    "\n",
    "\n",
    "\n",
    "test_dataloader = dict(\n",
    "    dataset=dict(\n",
    "        data_root='{dataset_location}',\n",
    "    ),)\n",
    "test_evaluator = dict(\n",
    "    ann_file='{dataset_location}/valid/_annotations.coco.json',)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = dict(\n",
    "\n",
    "    dataset=dict(\n",
    "        data_root='{dataset_location}',\n",
    "    ),)\n",
    "\n",
    "val_dataloader = dict(\n",
    "\n",
    "    dataset=dict(\n",
    "\n",
    "        data_root='{dataset_location}',\n",
    "    ),)\n",
    "    \n",
    "    \n",
    "val_evaluator = dict(\n",
    "    ann_file=\n",
    "    '{dataset_location}/valid/_annotations.coco.json',\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "with open(CUSTOM_CONFIG_PATH, 'w') as file:\n",
    "    file.write(CUSTOM_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe8b936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/11 00:58:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 42\n",
      "    GPU 0: Tesla T4\n",
      "    CUDA_HOME: /shared/centos7/cuda/11.2\n",
      "    NVCC: Cuda compilation tools, release 11.2, V11.2.67\n",
      "    GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\n",
      "    PyTorch: 2.0.0+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.15.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.10.2\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: True\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 42\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "03/11 00:58:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=16, enable=False)\n",
      "backend_args = None\n",
      "base_lr = 0.004\n",
      "batch_shapes_cfg = dict(\n",
      "    batch_size=8,\n",
      "    extra_pad_ratio=0.5,\n",
      "    img_size=1024,\n",
      "    size_divisor=32,\n",
      "    type='BatchShapePolicy')\n",
      "class_name = (\n",
      "    'Alive Tree',\n",
      "    'Beetle-Fire Tree',\n",
      "    'Dead Tree',\n",
      "    'Debris',\n",
      ")\n",
      "classes = (\n",
      "    'Alive Tree',\n",
      "    'Beetle-Fire Tree',\n",
      "    'Dead Tree',\n",
      "    'Debris',\n",
      ")\n",
      "data = dict(\n",
      "    samples_per_gpu=8,\n",
      "    test=dict(\n",
      "        ann_file='test/_annotations.coco.json',\n",
      "        classes=(\n",
      "            'Alive Tree',\n",
      "            'Beetle-Fire Tree',\n",
      "            'Dead Tree',\n",
      "            'Debris',\n",
      "        ),\n",
      "        img_prefix='test/',\n",
      "        type='COCODataset'),\n",
      "    train=dict(\n",
      "        ann_file='train/_annotations.coco.json.json',\n",
      "        classes=(\n",
      "            'Alive Tree',\n",
      "            'Beetle-Fire Tree',\n",
      "            'Dead Tree',\n",
      "            'Debris',\n",
      "        ),\n",
      "        img_prefix='train/',\n",
      "        type='COCODataset'),\n",
      "    val=dict(\n",
      "        ann_file='valid/_annotations.coco.json',\n",
      "        classes=(\n",
      "            'Alive Tree',\n",
      "            'Beetle-Fire Tree',\n",
      "            'Dead Tree',\n",
      "            'Debris',\n",
      "        ),\n",
      "        img_prefix='valid/',\n",
      "        type='COCODataset'))\n",
      "data_root = '/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1-lab'\n",
      "dataset_type = 'COCODataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(\n",
      "        interval=1,\n",
      "        max_keep_ckpts=10,\n",
      "        rule='greater',\n",
      "        save_best='coco/bbox_mAP_50',\n",
      "        type='CheckpointHook'),\n",
      "    early_stopping=dict(\n",
      "        min_delta=0.001,\n",
      "        monitor='coco/bbox_mAP_50',\n",
      "        patience=20,\n",
      "        type='EarlyStoppingHook'),\n",
      "    logger=dict(interval=50, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "dsl_topk = 13\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=True,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "evaluation = dict(interval=1, metric='bbox', save_best='bbox_mAP')\n",
      "img_scale = (\n",
      "    1024,\n",
      "    1024,\n",
      ")\n",
      "launcher = 'none'\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "loss_bbox_weight = 2.0\n",
      "loss_cls_weight = 1.0\n",
      "lr_start_factor = 1e-05\n",
      "max_epochs = 300\n",
      "max_keep_ckpts = 3\n",
      "metainfo = dict(\n",
      "    classes=(\n",
      "        'Alive Tree',\n",
      "        'Beetle-Fire Tree',\n",
      "        'Dead Tree',\n",
      "        'Debris',\n",
      "    ),\n",
      "    palette=[\n",
      "        (\n",
      "            20,\n",
      "            220,\n",
      "            60,\n",
      "        ),\n",
      "    ])\n",
      "mixup_max_cached_images = 20\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=50,\n",
      "        frozen_stages=1,\n",
      "        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),\n",
      "        norm_cfg=dict(requires_grad=True, type='BN'),\n",
      "        norm_eval=True,\n",
      "        num_stages=4,\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        style='pytorch',\n",
      "        type='ResNet'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_size_divisor=32,\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='DetDataPreprocessor'),\n",
      "    neck=dict(\n",
      "        in_channels=[\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            2048,\n",
      "        ],\n",
      "        num_outs=5,\n",
      "        out_channels=256,\n",
      "        type='FPN'),\n",
      "    roi_head=dict(\n",
      "        bbox_head=dict(\n",
      "            bbox_coder=dict(\n",
      "                target_means=[\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                ],\n",
      "                target_stds=[\n",
      "                    0.1,\n",
      "                    0.1,\n",
      "                    0.2,\n",
      "                    0.2,\n",
      "                ],\n",
      "                type='DeltaXYWHBBoxCoder'),\n",
      "            fc_out_channels=1024,\n",
      "            in_channels=256,\n",
      "            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "            loss_cls=dict(\n",
      "                loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=False),\n",
      "            num_classes=4,\n",
      "            reg_class_agnostic=False,\n",
      "            roi_feat_size=7,\n",
      "            type='Shared2FCBBoxHead'),\n",
      "        bbox_roi_extractor=dict(\n",
      "            featmap_strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            out_channels=256,\n",
      "            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),\n",
      "            type='SingleRoIExtractor'),\n",
      "        type='StandardRoIHead'),\n",
      "    rpn_head=dict(\n",
      "        anchor_generator=dict(\n",
      "            ratios=[\n",
      "                0.5,\n",
      "                1.0,\n",
      "                2.0,\n",
      "            ],\n",
      "            scales=[\n",
      "                8,\n",
      "            ],\n",
      "            strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "                64,\n",
      "            ],\n",
      "            type='AnchorGenerator'),\n",
      "        bbox_coder=dict(\n",
      "            target_means=[\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "            ],\n",
      "            target_stds=[\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "            ],\n",
      "            type='DeltaXYWHBBoxCoder'),\n",
      "        feat_channels=256,\n",
      "        in_channels=256,\n",
      "        loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "        loss_cls=dict(\n",
      "            loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=True),\n",
      "        type='RPNHead'),\n",
      "    test_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            max_per_img=100,\n",
      "            nms=dict(iou_threshold=0.5, type='nms'),\n",
      "            score_thr=0.05),\n",
      "        rpn=dict(\n",
      "            max_per_img=1000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=1000)),\n",
      "    train_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=False,\n",
      "                min_pos_iou=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                pos_iou_thr=0.5,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=True,\n",
      "                neg_pos_ub=-1,\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                type='RandomSampler')),\n",
      "        rpn=dict(\n",
      "            allowed_border=-1,\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.3,\n",
      "                neg_iou_thr=0.3,\n",
      "                pos_iou_thr=0.7,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=False,\n",
      "                neg_pos_ub=-1,\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                type='RandomSampler')),\n",
      "        rpn_proposal=dict(\n",
      "            max_per_img=1000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=2000)),\n",
      "    type='FasterRCNN')\n",
      "model_test_cfg = dict(\n",
      "    max_per_img=300,\n",
      "    multi_label=True,\n",
      "    nms=dict(iou_threshold=0.65, type='nms'),\n",
      "    nms_pre=30000,\n",
      "    score_thr=0.001)\n",
      "mosaic_max_cached_images = 40\n",
      "num_classes = 4\n",
      "num_epochs_stage2 = 20\n",
      "optim_wrapper = dict(\n",
      "    optimizer=dict(lr=0.02, momentum=0.9, type='SGD', weight_decay=0.0001),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0, by_epoch=False, end=500, start_factor=0.001, type='LinearLR'),\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        end=12,\n",
      "        gamma=0.1,\n",
      "        milestones=[\n",
      "            8,\n",
      "            11,\n",
      "        ],\n",
      "        type='MultiStepLR'),\n",
      "]\n",
      "persistent_workers = True\n",
      "qfl_beta = 2.0\n",
      "random_resize_ratio_range = (\n",
      "    0.1,\n",
      "    2.0,\n",
      ")\n",
      "randomness = dict(seed=42)\n",
      "resume = True\n",
      "save_checkpoint_intervals = 10\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='valid/_annotations.coco.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='valid/'),\n",
      "        data_root='/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1-lab',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file=\n",
      "    '/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1-lab/valid/_annotations.coco.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_ann_file = 'train/_annotations.coco.json'\n",
      "train_batch_size_per_gpu = 8\n",
      "train_cfg = dict(max_epochs=300, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_data_prefix = 'train/'\n",
      "train_dataloader = dict(\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file='train/_annotations.coco.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='train/'),\n",
      "        data_root='/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1-lab',\n",
      "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackDetInputs'),\n",
      "        ],\n",
      "        type='CocoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackDetInputs'),\n",
      "]\n",
      "val_ann_file = 'valid/_annotations.coco.json'\n",
      "val_batch_size_per_gpu = 8\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_data_prefix = 'valid/'\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='valid/_annotations.coco.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='valid/'),\n",
      "        data_root='/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1-lab',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file=\n",
      "    '/work/van-speech-nlp/jindaznb/j-vis/Forestfire2024-1-lab/valid/_annotations.coco.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "val_interval_stage2 = 1\n",
      "val_num_workers = 3\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "weight_decay = 0.05\n",
      "work_dir = './work_dirs/custom-faster-rcnn_r50_fpn_1x_coco-lab'\n",
      "\n",
      "03/11 00:59:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "03/11 00:59:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOWEST      ) EarlyStoppingHook                  \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      "(LOWEST      ) EarlyStoppingHook                  \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "03/11 00:59:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - load model from: torchvision://resnet50\n",
      "03/11 00:59:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by torchvision backend from path: torchvision://resnet50\n",
      "03/11 00:59:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n",
      "03/11 00:59:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Auto resumed from the latest checkpoint /work/van-speech-nlp/jindaznb/j-vis/mmdetection/work_dirs/custom-faster-rcnn_r50_fpn_1x_coco-lab/epoch_199.pth.\n",
      "Loads checkpoint by local backend from path: /work/van-speech-nlp/jindaznb/j-vis/mmdetection/work_dirs/custom-faster-rcnn_r50_fpn_1x_coco-lab/epoch_199.pth\n",
      "03/11 00:59:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from /work/van-speech-nlp/jindaznb/j-vis/mmdetection/work_dirs/custom-faster-rcnn_r50_fpn_1x_coco-lab/epoch_199.pth\n",
      "03/11 00:59:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The value of random seed in the checkpoint \"484897047\" is different from the value in `randomness` config \"42\"\n",
      "03/11 00:59:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - resumed epoch: 199, iter: 46964\n",
      "03/11 00:59:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "03/11 00:59:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "03/11 00:59:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /work/van-speech-nlp/jindaznb/j-vis/mmdetection/work_dirs/custom-faster-rcnn_r50_fpn_1x_coco-lab.\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/tools/train.py\", line 121, in <module>\n",
      "    main()\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/tools/train.py\", line 117, in main\n",
      "    runner.train()\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/runner/runner.py\", line 1777, in train\n",
      "    model = self.train_loop.run()  # type: ignore\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/runner/loops.py\", line 96, in run\n",
      "    self.run_epoch()\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/runner/loops.py\", line 112, in run_epoch\n",
      "    self.run_iter(idx, data_batch)\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/runner/loops.py\", line 128, in run_iter\n",
      "    outputs = self.runner.model.train_step(\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py\", line 114, in train_step\n",
      "    losses = self._run_forward(data, mode='loss')  # type: ignore\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py\", line 346, in _run_forward\n",
      "    results = self(**data, mode=mode)\n",
      "  File \"/work/van-speech-nlp/jenv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/detectors/base.py\", line 92, in forward\n",
      "    return self.loss(inputs, data_samples)\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/detectors/two_stage.py\", line 174, in loss\n",
      "    rpn_losses, rpn_results_list = self.rpn_head.loss_and_predict(\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/dense_heads/base_dense_head.py\", line 165, in loss_and_predict\n",
      "    losses = self.loss_by_feat(*loss_inputs)\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/dense_heads/rpn_head.py\", line 125, in loss_by_feat\n",
      "    losses = super().loss_by_feat(\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/dense_heads/anchor_head.py\", line 502, in loss_by_feat\n",
      "    cls_reg_targets = self.get_targets(\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/dense_heads/anchor_head.py\", line 378, in get_targets\n",
      "    results = multi_apply(\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/utils/misc.py\", line 219, in multi_apply\n",
      "    return tuple(map(list, zip(*map_results)))\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/dense_heads/anchor_head.py\", line 252, in _get_targets_single\n",
      "    assign_result = self.assigner.assign(pred_instances, gt_instances,\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/task_modules/assigners/max_iou_assigner.py\", line 220, in assign\n",
      "    overlaps = self.iou_calculator(gt_bboxes_unique, priors)\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/models/task_modules/assigners/iou2d_calculator.py\", line 62, in __call__\n",
      "    return bbox_overlaps(bboxes1, bboxes2, mode, is_aligned)\n",
      "  File \"/work/van-speech-nlp/jindaznb/j-vis/mmdetection/mmdet/structures/bbox/bbox_overlaps.py\", line 176, in bbox_overlaps\n",
      "    wh = fp16_clamp(rb - lt, min=0)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 266.00 MiB (GPU 0; 14.58 GiB total capacity; 2.33 GiB already allocated; 236.56 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python tools/train.py configs/{model_folder}/{custom_model_name}.py \\\n",
    "    --resume --cfg-options randomness.seed=42 \\\n",
    "    | tee \"../log/OUT_{custom_model_name}_$(date +\"%Y%m%d_%H%M%S\").txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5ff9a7-82e1-466c-8953-35de0a006b34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m CONFIDENCE_THRESHOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      2\u001b[0m NMS_IOU_THRESHOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m----> 4\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43msv\u001b[49m\u001b[38;5;241m.\u001b[39mDetectionDataset\u001b[38;5;241m.\u001b[39mfrom_coco(\n\u001b[1;32m      5\u001b[0m     images_directory_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     annotations_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test/_annotations.coco.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ds\u001b[38;5;241m.\u001b[39mimages\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     10\u001b[0m CUSTOM_WEIGHTS_PATH \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/mmdetection/work_dirs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/best_coco_bbox_mAP_50_epoch_*.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sv' is not defined"
     ]
    }
   ],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "NMS_IOU_THRESHOLD = 0.5\n",
    "\n",
    "ds = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset_location}/test\",\n",
    "    annotations_path=f\"{dataset_location}/test/_annotations.coco.json\",\n",
    ")\n",
    "\n",
    "images = list(ds.images.values())\n",
    "CUSTOM_WEIGHTS_PATH = glob.glob(f\"{HOME}/mmdetection/work_dirs/{model_folder}/best_coco_bbox_mAP_50_epoch_*.pth\")[-1]\n",
    "model = init_detector(CUSTOM_CONFIG_PATH, CUSTOM_WEIGHTS_PATH, device=DEVICE)\n",
    "\n",
    "def callback(image: np.ndarray) -> sv.Detections:\n",
    "    result = inference_detector(model, image)\n",
    "    detections = sv.Detections.from_mmdetection(result)\n",
    "    return detections[detections.confidence > CONFIDENCE_THRESHOLD].with_nms(threshold=NMS_IOU_THRESHOLD)\n",
    "\n",
    "\n",
    "mean_average_precision = sv.MeanAveragePrecision.benchmark(\n",
    "    dataset = ds,\n",
    "    callback = callback\n",
    ")\n",
    "\n",
    "print('mAP:', mean_average_precision.map50_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366c722-c495-4b59-8f86-ba5ded5fbe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset_location}/test\",\n",
    "    annotations_path=f\"{dataset_location}/test/_annotations.coco.json\",\n",
    ")\n",
    "\n",
    "images = list(ds.images.values())\n",
    "CUSTOM_WEIGHTS_PATH = glob.glob(f\"{HOME}/mmdetection/work_dirs/{model_folder}/best_coco_bbox_mAP_50_epoch_*.pth\")[-1]\n",
    "model = init_detector(CUSTOM_CONFIG_PATH, CUSTOM_WEIGHTS_PATH, device=DEVICE)\n",
    "\n",
    "best_map, best_params = optimize_params(ds, model, param_space)\n",
    "print('Best mAP:', best_map)\n",
    "print('Best Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52a9aa-b187-4d2b-b64a-067380bc7b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb13ae-9724-473a-a1dd-733d6fdaf8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced3453-1687-4c69-b055-85dfba034281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453232e4-683e-4c41-9975-5639b643af04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
